{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T23:16:23.617553Z",
     "start_time": "2025-04-25T23:16:23.612848Z"
    }
   },
   "source": [
    "# Dataset Directory\n",
    "\n",
    "dataset_folder = '/mnt/Data1/Nick/transcription_pipeline/'\n",
    "\n",
    "RBSPWM_datasets = [\n",
    "    \"test_data/2024-02-26/Halo-RBSPWM_embryo01\",\n",
    "    \"test_data/2024-02-26/Halo-RBSPWM_embryo02\",\n",
    "    \"test_data/2024-05-07/Halo552-RBSPWM_embryo01\",\n",
    "    \"test_data/2024-05-07/Halo552-RBSPWM_embryo02\",\n",
    "    \"test_data/2024-05-09/Halo552-RBSPWM_embryo01\",\n",
    "]\n",
    "\n",
    "RBSVar2_datasets = [\n",
    "    \"test_data/2024-07-23/Halo673_RBSVar2_embryo01\",\n",
    "    \"test_data/2024-07-25/Halo673_RBSVar2_embryo01\",\n",
    "    \"test_data/2024-10-10/Halo673_RBSVar2_embryo01\",\n",
    "    \"test_data/2024-10-10/Halo673_RBSVar2_embryo02\",\n",
    "]\n",
    "MCP_mSG_datasets = [\n",
    "    \"test_data/2024-10-31/MCP-mSG_ParB-mScar_RBSPWM_embryo01\",\n",
    "    \"test_data/2024-10-31/MCP-mSG_ParB-mScar_RBSPWM_embryo02\",\n",
    "    \"test_data/2025-03-18/MCP-mSG_His-RFP_RBSPWM(003)_embryo01\",\n",
    "    \"test_data/2025-03-18/MCP-mSG_His-RFP_RBSPWM(003)_embryo02\",\n",
    "    ]\n",
    "\n",
    "NSPARC_datasets = [\n",
    "    'test_data/NSPARC/2025-03-17/MCP-Halo552_His-GFP_Var2(001)_embryo01',\n",
    "    'test_data/NSPARC/2025-03-31/MCP-mSG_His-RFP_Var2(001)_embryo01',\n",
    "    'test_data/NSPARC/2025-03-31/MCP-mSG_His-RFP_Var2(001)_embryo02',\n",
    "    'test_data/NSPARC/2025-04-01/MCP-mSG_His-RFP_Var2(001)_embryo20',\n",
    "    'test_data/NSPARC/2025-04-01/MCP-mSG_His-RFP_Var2(001)_embryo38',\n",
    "    'test_data/NSPARC/2025-04-14/MCP-mSG_His-RFP_Var2(001)_embryo28',\n",
    "    'test_data/NSPARC/2025-04-15/MCP-mSG_His-RFP_Var2(001)_embryo01',\n",
    "]\n",
    "test_dataset_name = dataset_folder + NSPARC_datasets[5]\n",
    "print('Dataset Path: ' + test_dataset_name)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Path: /mnt/Data1/Nick/transcription_pipeline/test_data/NSPARC/2025-04-14/MCP-mSG_His-RFP_Var2(001)_embryo28\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T23:16:27.432888Z",
     "start_time": "2025-04-25T23:16:26.446144Z"
    }
   },
   "source": [
    "# Import pipeline\n",
    "from transcription_pipeline import nuclear_pipeline\n",
    "from transcription_pipeline import preprocessing_pipeline\n",
    "\n",
    "from transcription_pipeline import spot_pipeline\n",
    "from transcription_pipeline import fullEmbryo_pipeline\n",
    "\n",
    "from transcription_pipeline.spot_analysis import compile_data\n",
    "from transcription_pipeline.utils import plottable\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`JAVA_HOME` environment variable set to /home/nickgravina/miniforge3/envs/transcription_pipeline\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T23:16:31.602924Z",
     "start_time": "2025-04-25T23:16:31.592052Z"
    }
   },
   "source": [
    "# Specify how you would want the plots to be shown: Use TkAgg if you use PyCharm, or widget if you use a browser\n",
    "\n",
    "mpl.use('TkAgg')\n",
    "# %matplotlib widget"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T23:16:34.267266Z",
     "start_time": "2025-04-25T23:16:34.262372Z"
    }
   },
   "source": [
    "ms2_import_previous = os.path.isdir(test_dataset_name + '/collated_dataset')\n",
    "ms2_import_previous"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T23:16:39.192143Z",
     "start_time": "2025-04-25T23:16:37.467407Z"
    }
   },
   "source": [
    "dataset = preprocessing_pipeline.DataImport(\n",
    "    name_folder=test_dataset_name,\n",
    "    trim_series=True,\n",
    "    working_storage_mode='zarr',\n",
    "    import_previous=ms2_import_previous, \n",
    ")\n",
    "if not ms2_import_previous:\n",
    "    dataset.save()"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset.export_frame_metadata[nuclear_channel]['t_s'].shape",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import FullEmbryo Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "FullEmbryo_dataset = preprocessing_pipeline.FullEmbryoImport(\n",
    "    name_folder=test_dataset_name,\n",
    "    import_previous=True\n",
    ")\n",
    "# Loading FullEmbryo dataset is not working currently, but reported to Yovan where it only reads in the last channel\n",
    "# FullEmbryo_dataset.save()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting a DASK Client for parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from dask.distributed import LocalCluster, Client\n",
    "\n",
    "try:\n",
    "    cluster = LocalCluster(\n",
    "        host=\"localhost\",\n",
    "        #scheduler_port=37763,\n",
    "        threads_per_worker=1,\n",
    "        n_workers=14,\n",
    "        memory_limit=\"6GB\",\n",
    "    )\n",
    "    \n",
    "    client = Client(cluster)\n",
    "except:\n",
    "    print(\"Cluster already running\")\n",
    "    client = Client('localhost:37763')\n",
    "\n",
    "print(client)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "client.restart()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "client.dashboard_link",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuclear Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect whether the nuclear tracking has been done \"previously.\" If so, load the previous results."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot dataset\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(dataset.channels_full_dataset[1][49,5, :, :], cmap='gray')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nuclear_channel = 1\n",
    "spot_channel = 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "nuclear_tracking_previous = os.path.isdir(test_dataset_name + '/nuclear_analysis_results')\n",
    "nuclear_tracking_previous"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if nuclear_tracking_previous:\n",
    "    # Load nuclear tracking results\n",
    "    print('Load from previous nuclear tracking results')\n",
    "    \n",
    "    nuclear_tracking = nuclear_pipeline.Nuclear()\n",
    "    nuclear_tracking.read_results(name_folder=test_dataset_name)\n",
    "    \n",
    "else:\n",
    "    # Do nuclear tracking and save the results\n",
    "    print('Do nuclear tracking for the dataset')\n",
    "    \n",
    "    nuclear_tracking = nuclear_pipeline.Nuclear(\n",
    "        data=dataset.channels_full_dataset[nuclear_channel],\n",
    "        global_metadata=dataset.export_global_metadata[nuclear_channel],\n",
    "        frame_metadata=dataset.export_frame_metadata[nuclear_channel],\n",
    "        series_splits=dataset.series_splits,\n",
    "        series_shifts=dataset.series_shifts,\n",
    "        search_range_um=1.5,\n",
    "        stitch=False,\n",
    "        stitch_max_distance=4,\n",
    "        stitch_max_frame_distance=2,\n",
    "        client=client,\n",
    "        keep_futures=False,\n",
    "    )\n",
    "    \n",
    "    nuclear_tracking.track_nuclei(\n",
    "            working_memory_mode=\"zarr\",\n",
    "            working_memory_folder=test_dataset_name,\n",
    "            trackpy_log_path=\"\".join([test_dataset_name, \"trackpy_log\"]),\n",
    "        )\n",
    "        # Saves tracked nuclear mask as a zarr, and pickles dataframes with segmentation and\n",
    "        # tracking information.\n",
    "    nuclear_tracking.save_results(\n",
    "            name_folder=test_dataset_name, save_array_as=None\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spot Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect whether the spot tracking has been done \"previously.\" If so, load the previous results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "spot_tracking_previous = os.path.isdir(test_dataset_name + '/spot_analysis_results')\n",
    "spot_tracking_previous"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "if spot_tracking_previous:\n",
    "    # Load spot tracking results\n",
    "    print('Load from spot tracking results')\n",
    "    \n",
    "    spot_tracking = spot_pipeline.Spot()\n",
    "    spot_tracking.read_results(name_folder=test_dataset_name)\n",
    "    \n",
    "else:\n",
    "    # Do spot tracking and save the results\n",
    "    print('Do spot tracking for the dataset')\n",
    "    \n",
    "    spot_tracking = spot_pipeline.Spot(\n",
    "        data=dataset.channels_full_dataset[spot_channel],\n",
    "        global_metadata=dataset.export_global_metadata[spot_channel],\n",
    "        frame_metadata=dataset.export_frame_metadata[spot_channel],\n",
    "        labels=nuclear_tracking.reordered_labels,\n",
    "        expand_distance=3,\n",
    "        search_range_um=4.2,\n",
    "        retrack_search_range_um=4.5,\n",
    "        threshold_factor=1.3,\n",
    "        memory=3,\n",
    "        retrack_after_filter=False,\n",
    "        stitch=True,\n",
    "        min_track_length=0,\n",
    "        series_splits=dataset.series_splits,\n",
    "        series_shifts=dataset.series_shifts,\n",
    "        keep_bandpass=False,\n",
    "        keep_futures=False,\n",
    "        keep_spot_labels=False,\n",
    "        evaluate=True,\n",
    "        retrack_by_intensity=True,\n",
    "        client=client,\n",
    "    )\n",
    "    \n",
    "    spot_tracking.extract_spot_traces(\n",
    "        working_memory_folder=test_dataset_name, \n",
    "        stitch=True,\n",
    "        retrack_after_filter=True,\n",
    "        trackpy_log_path = test_dataset_name+'/trackpy_log'\n",
    "    )\n",
    "    \n",
    "    # Saves tracked spot mask as a zarr, and pickles dataframes with spot fitting and\n",
    "    # quantification information.\n",
    "    spot_tracking.save_results(name_folder=test_dataset_name, save_array_as=None)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Compiled Dataframe"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "spot_tracking.reordered_spot_labels",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load spot tracking dataframe\n",
    "spot_df = spot_tracking.spot_dataframe\n",
    "\n",
    "# Remove spots that were not detected\n",
    "detected_spots = spot_df[spot_df[\"particle\"] != 0]\n",
    "\n",
    "# Compile traces\n",
    "compiled_dataframe = compile_data.compile_traces(\n",
    "    detected_spots,\n",
    "    compile_columns_spot=[\n",
    "        \"frame\",\n",
    "        \"t_s\",\n",
    "        \"intensity_from_neighborhood\",\n",
    "        \"intensity_std_error_from_neighborhood\",\n",
    "        \"x\",\n",
    "        \"y\"\n",
    "    ],\n",
    "    nuclear_tracking_dataframe=None,\n",
    ")\n",
    "\n",
    "compiled_dataframe.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from transcription_pipeline.gui import check_spots",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "check_spots.CheckSpotsGUI(\n",
    "    spot_channel=dataset.channels_full_dataset[spot_channel],\n",
    "    labels=spot_tracking.reordered_spot_labels,\n",
    "    dataset_name=test_dataset_name,\n",
    "    spot_channel_index=spot_channel,\n",
    "    compiled_dataframe=compiled_dataframe,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Embryo Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(FullEmbryo_dataset.channels_full_dataset_surf[0][0, :, :], cmap='gray')\n",
    "plt.title('Full Embryo Surf')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(FullEmbryo_dataset.channels_full_dataset_mid[0][0, :, :], cmap='gray')\n",
    "plt.title('Full Embryo Mid')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib\n",
    "from skimage import color, filters, morphology, util, measure, transform#, exposure, segmentation, io\n",
    "from scipy.spatial import ConvexHull\n",
    "from skimage.draw import line\n",
    "import feret\n",
    "import cv2 as cv\n",
    "from skimage.transform import AffineTransform\n",
    "\n",
    "def contour_mask(binary_mask):\n",
    "    \"\"\"\n",
    "    Generates a mask by flood filling the largest contour within the input binary_mask.\n",
    "    \"\"\"\n",
    "    contours = measure.find_contours(binary_mask)\n",
    "\n",
    "    # Identify the desired contour (e.g., the largest)\n",
    "    largest_contour = max(contours, key=len)\n",
    "\n",
    "    # Fit a convex hull to the contour\n",
    "    hull = ConvexHull(largest_contour)\n",
    "\n",
    "    # Initialize the FullEmbryo mask\n",
    "    mask = np.zeros(binary_mask.shape)\n",
    "\n",
    "    # Extract points from the contour\n",
    "    pts0 = [(largest_contour[simplex, 1][0], largest_contour[simplex, 0][0]) for simplex in hull.simplices]\n",
    "    pts1 = [(largest_contour[simplex, 1][1], largest_contour[simplex, 0][1]) for simplex in hull.simplices]\n",
    "    pts = pts0 + pts1\n",
    "    pts = np.array(pts)\n",
    "\n",
    "    # Calculate reference point for determining polar angle\n",
    "    reference_point = np.mean(pts, axis=0)\n",
    "\n",
    "    # Function to calculate the polar angle relative to a reference point\n",
    "    def polar_angle(point):\n",
    "        x, y = point[0] - reference_point[0], point[1] - reference_point[1]\n",
    "        return np.arctan2(y, x)\n",
    "\n",
    "    # Sort points based on polar angle\n",
    "    sorted_pts = sorted(pts, key=polar_angle)\n",
    "\n",
    "    # Draw contour connecting sorted points\n",
    "    for i in range(len(sorted_pts)):\n",
    "        if i == len(sorted_pts) - 1:\n",
    "            x1, y1 = np.round(sorted_pts[i])\n",
    "            x2, y2 = np.round(sorted_pts[0])\n",
    "        else:\n",
    "            x1, y1 = np.round(sorted_pts[i])\n",
    "            x2, y2 = np.round(sorted_pts[i + 1])\n",
    "\n",
    "        x1 = int(x1)\n",
    "        y1 = int(y1)\n",
    "        x2 = int(x2)\n",
    "        y2 = int(y2)\n",
    "\n",
    "        rr, cc = line(y1, x1, y2, x2)\n",
    "        mask[rr, cc] = 1\n",
    "\n",
    "    # Save contour mask\n",
    "    contour_mask = mask\n",
    "\n",
    "    # Flood fill to generate the FullEmbryo mask\n",
    "    mask = morphology.flood_fill(mask, (0, 0), 1, connectivity=1)\n",
    "    mask = util.invert(mask)\n",
    "\n",
    "    return mask, contour_mask\n",
    "\n",
    "def gen_full_embryo_mask(tif_array, sigma=10, radius=5):\n",
    "    \"\"\"\n",
    "    Creates a FullEmbryo mask by detecting the embryo edge through a Gaussian blur, thresholding, and a closing operation.\n",
    "    \"\"\"\n",
    "    # Convert the image to grayscale if it's not already\n",
    "    # if tif_array.shape[-1] == 3:\n",
    "    #     grayscale_image = color.rgb2gray(tif_array)\n",
    "    # else:\n",
    "    #     grayscale_image = tif_array\n",
    "\n",
    "    # Gaussian blur the image with given sigma\n",
    "    tif_array = filters.gaussian(tif_array, sigma)\n",
    "\n",
    "    # Otsu thresholding\n",
    "    threshold_value = filters.threshold_otsu(tif_array)\n",
    "    tif_array = tif_array > 1 * threshold_value\n",
    "\n",
    "    # Closing with disk of given radius\n",
    "    tif_array = morphology.closing(tif_array, morphology.disk(radius))\n",
    "\n",
    "    mask, contour = contour_mask(tif_array)\n",
    "    return mask, contour"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image = FullEmbryo_dataset.channels_full_dataset_mid[nuclear_channel][1, :, :]\n",
    "\n",
    "# Threshold the image\n",
    "otsu_threshold = filters.threshold_otsu(image)\n",
    "print(otsu_threshold)\n",
    "binary_image =image < otsu_threshold\n",
    "final_image = np.where(binary_image, image, 0)\n",
    "\n",
    "\n",
    "mask, _ = gen_full_embryo_mask(tif_array=image, sigma=10, radius=5)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(final_image, cmap='gray')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mask, cmap='gray')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "fullEmbryo = fullEmbryo_pipeline.FullEmbryo(FullEmbryo_dataset, dataset, his_channel=nuclear_channel)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "fullEmbryo.find_ap_axis(make_plots=True, remove_small_objects=False, ap_method='minf90', sigma=10, radius=5)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "fullEmbryo.swap_ap_points(make_plots=True)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "compiled_dataframe = fullEmbryo.xy_to_ap(compiled_dataframe)\n",
    "compiled_dataframe.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save compiled_dataframe as pickle\n",
    "compiled_dataframe.to_pickle(test_dataset_name + '/compiled_dataframe.pkl')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## RateExtraction Analysis\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and Average"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from transcription_pipeline.RateExtraction import FitAndAverage",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Specify here at what frame NC14 starts\n",
    "nc14_start_frame = 0\n",
    "\n",
    "# Any trace with frame number smaller than min_frames will be filtered out\n",
    "min_frames = 40\n",
    "\n",
    "# Number of bins you want to split the full embryo into\n",
    "num_bins = 42"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "faadata = FitAndAverage(compiled_dataframe, nc14_start_frame, min_frames, num_bins, test_dataset_name)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "faadata.check_particle_fits()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "faadata.save_checked_particle_fits()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "faadata.average_particle_fits(plot_results=True, show_slopes=True);",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "faax, faay, faay_err, _, _ = faadata.average_particle_fits();",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Average and Fit (using approved particle fits)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "faadatapoints = pd.read_pickle(faadata.checked_particle_fits_file_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transcription_pipeline.RateExtraction import AverageAndFit\n",
    "time_bin_width = dataset.export_frame_metadata[0]['t_s'][1, 0]\n",
    "aafdata_sp = AverageAndFit(faadatapoints, nc14_start_frame-3, time_bin_width, num_bins, test_dataset_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "aafdata_sp.check_bin_fits()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "aafdata_sp.bin_average_fit_dataframe;",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "aafdata_sp.save_checked_bin_fits()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "aafspx, aafspy, aafspy_err = aafdata_sp.plot_bin_fits()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Average and Fit"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from transcription_pipeline.RateExtraction import AverageAndFit"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "time_bin_width = dataset.export_frame_metadata[0]['t_s'][1, 0]\n",
    "aafdata = AverageAndFit(compiled_dataframe, nc14_start_frame, time_bin_width, num_bins, test_dataset_name);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "aafdata.check_bin_fits()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "aafdata.bin_average_fit_dataframe"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "aafdata.save_checked_bin_fits()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "aafx, aafy, aafy_err = aafdata.plot_bin_fits()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.errorbar(faax, faay, yerr=faay_err, capsize=2, fmt='o', label='faa')\n",
    "plt.errorbar(aafspx, aafspy, yerr=aafspy_err, capsize=2, fmt='o', label='aaf_sp')\n",
    "plt.errorbar(aafx, aafy, yerr=aafy_err, capsize=2, fmt='o', label='aaf')\n",
    "\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
