{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset and Specify Parameters (please only edit cells in this section)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T23:51:13.611800Z",
     "start_time": "2025-03-24T23:51:13.609482Z"
    }
   },
   "source": [
    "# Specify here which steps you want to run below\n",
    "\n",
    "spot_tracking = True\n",
    "\n",
    "nuclear_tracking = True"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T23:51:14.607178Z",
     "start_time": "2025-03-24T23:51:14.603258Z"
    }
   },
   "source": [
    "dataset_folder = '/mnt/Data1/Nick/transcription_pipeline/test_data/2025-03-18/'\n",
    "\n",
    "import os\n",
    "folder_list = sorted([entry.name for entry in os.scandir(dataset_folder) if entry.is_dir()])\n",
    "folder_list"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dl-Dendra2_fullEmbryo',\n",
       " 'MCP-Halo552_His-BFP_r1close(002)_StillImages',\n",
       " 'MCP-mSG_His-RFP_RBSPWM(003)_embryo01',\n",
       " 'MCP-mSG_His-RFP_RBSPWM(003)_embryo02',\n",
       " 'MCP-mSG_His-RFP_RBSPWM(003)_embryo03',\n",
       " 'MCP-mSG_His-RFP_RBSPWM(003)_embryo04',\n",
       " 'MCP-mSG_His-RFP_RBSPWM(003)_embryo05']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the data sets you want to analyze (edit the cell below)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T23:51:21.382582Z",
     "start_time": "2025-03-24T23:51:21.379662Z"
    }
   },
   "source": [
    "dataset_paths = folder_list[4:7] # Specify here what data sets you want to analyze\n",
    "dataset_paths"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MCP-mSG_His-RFP_RBSPWM(003)_embryo03',\n",
       " 'MCP-mSG_His-RFP_RBSPWM(003)_embryo04',\n",
       " 'MCP-mSG_His-RFP_RBSPWM(003)_embryo05']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T23:51:22.561858Z",
     "start_time": "2025-03-24T23:51:22.559400Z"
    }
   },
   "source": [
    "# Import pipeline\n",
    "from transcription_pipeline import nuclear_pipeline\n",
    "from transcription_pipeline import preprocessing_pipeline\n",
    "\n",
    "from transcription_pipeline import spot_pipeline\n",
    "from transcription_pipeline import fullEmbryo_pipeline\n",
    "\n",
    "from transcription_pipeline.spot_analysis import compile_data\n",
    "from transcription_pipeline.utils import plottable\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting a DASK Client for parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T23:51:24.194950Z",
     "start_time": "2025-03-24T23:51:24.168426Z"
    }
   },
   "source": [
    "from dask.distributed import LocalCluster, Client\n",
    "\n",
    "try:\n",
    "    cluster = LocalCluster(\n",
    "        host=\"localhost\",\n",
    "        scheduler_port=37763,\n",
    "        threads_per_worker=1,\n",
    "        n_workers=14,\n",
    "        memory_limit=\"6GB\",\n",
    "    )\n",
    "    \n",
    "    client = Client(cluster)\n",
    "except:\n",
    "    print(\"Cluster already running\")\n",
    "    client = Client('localhost:37763')\n",
    "\n",
    "print(client)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Data1/Nick/miniforge3/envs/transcription_pipeline/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 44557 instead\n",
      "  warnings.warn(\n",
      "2025-03-24 16:51:24,186 - distributed.deploy.spec - WARNING - Cluster closed without starting up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster already running\n",
      "<Client: 'tcp://127.0.0.1:37763' processes=14 threads=14, memory=78.23 GiB>\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T23:51:27.041085Z",
     "start_time": "2025-03-24T23:51:24.993550Z"
    }
   },
   "source": [
    "client.restart()\n",
    "# client.shutdown()\n",
    "print(client.dashboard_link)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8787/status\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop over all the data sets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-03-24T23:51:29.323797Z"
    }
   },
   "source": [
    "for i in range(len(dataset_paths)):\n",
    "    \n",
    "    #------------------------------\n",
    "    try:\n",
    "        test_dataset_name = dataset_folder + dataset_paths[i]\n",
    "        print('Process data for: ' + test_dataset_name)\n",
    "\n",
    "        # Import MS2 dataset\n",
    "        # Detect whether the MS2 dataset has already been converted into `zarr` files\n",
    "        ms2_import_previous = os.path.isdir(test_dataset_name + '/collated_dataset')\n",
    "        print('Import MS2 from previous zarr files: ' + str(ms2_import_previous))\n",
    "    \n",
    "        dataset = preprocessing_pipeline.DataImport(\n",
    "            name_folder=test_dataset_name,\n",
    "            trim_series=True,\n",
    "            working_storage_mode='zarr',\n",
    "            import_previous=ms2_import_previous, \n",
    "        )\n",
    "\n",
    "        if not ms2_import_previous:\n",
    "            dataset.save()\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "\n",
    "    \n",
    "    #------------------------------\n",
    "    try:\n",
    "        # Import FullEmbryo dataset\n",
    "        # Detect whether the FullEmbryo dataset has already been converted into `zarr` files\n",
    "        fullembryo_import_previous = os.path.isdir(test_dataset_name + '/preprocessed_full_embryo')\n",
    "        print('Import full embryo from previous zarr files: ' + str(fullembryo_import_previous))\n",
    "    \n",
    "        FullEmbryo_dataset = preprocessing_pipeline.FullEmbryoImport(\n",
    "            name_folder=test_dataset_name,\n",
    "            import_previous=fullembryo_import_previous\n",
    "        )\n",
    "    \n",
    "        if not fullembryo_import_previous:\n",
    "            FullEmbryo_dataset.save()\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "    \n",
    "\n",
    "    #------------------------------\n",
    "    try:\n",
    "        # Nuclear Tracking\n",
    "        if nuclear_tracking:\n",
    "            # Detect whether the nuclear tracking has been done \"previously.\" If so, load the previous results.\n",
    "            nuclear_tracking_previous = os.path.isdir(test_dataset_name + '/nuclear_analysis_results')\n",
    "\n",
    "            if nuclear_tracking_previous:\n",
    "                # Load nuclear tracking results\n",
    "                print('Load from previous nuclear tracking results')\n",
    "                \n",
    "                nuclear_tracking = nuclear_pipeline.Nuclear()\n",
    "                nuclear_tracking.read_results(name_folder=test_dataset_name)\n",
    "                \n",
    "            else:\n",
    "                # Do nuclear tracking and save the results\n",
    "                print('Do nuclear tracking for the dataset')\n",
    "                \n",
    "                nuclear_tracking = nuclear_pipeline.Nuclear(\n",
    "                    data=dataset.channels_full_dataset[0],\n",
    "                    global_metadata=dataset.export_global_metadata[0],\n",
    "                    frame_metadata=dataset.export_frame_metadata[0],\n",
    "                    series_splits=dataset.series_splits,\n",
    "                    series_shifts=dataset.series_shifts,\n",
    "                    search_range_um=1.5,\n",
    "                    stitch=False,\n",
    "                    stitch_max_distance=4,\n",
    "                    stitch_max_frame_distance=2,\n",
    "                    client=client,\n",
    "                    keep_futures=False,\n",
    "                )\n",
    "                \n",
    "                nuclear_tracking.track_nuclei(\n",
    "                        working_memory_mode=\"zarr\",\n",
    "                        working_memory_folder=test_dataset_name,\n",
    "                        trackpy_log_path=\"\".join([test_dataset_name, \"trackpy_log\"]),\n",
    "                    )\n",
    "                    # Saves tracked nuclear mask as a zarr, and pickles dataframes with segmentation and\n",
    "                    # tracking information.\n",
    "                nuclear_tracking.save_results(\n",
    "                        name_folder=test_dataset_name, save_array_as=None\n",
    "                    )\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "\n",
    "\n",
    "    #------------------------------\n",
    "    try:\n",
    "        # Spot Tracking\n",
    "        if spot_tracking:\n",
    "            spot_tracking_previous = os.path.isdir(test_dataset_name + '/spot_analysis_results')\n",
    "\n",
    "            if spot_tracking_previous:\n",
    "                # Load spot tracking results\n",
    "                print('Load from spot tracking results')\n",
    "                \n",
    "                spot_tracking = spot_pipeline.Spot()\n",
    "                spot_tracking.read_results(name_folder=test_dataset_name)\n",
    "                \n",
    "            else:\n",
    "                # Do spot tracking and save the results\n",
    "                print('Do spot tracking for the dataset')\n",
    "                \n",
    "                spot_tracking = spot_pipeline.Spot(\n",
    "                    data=dataset.channels_full_dataset[1],\n",
    "                    global_metadata=dataset.export_global_metadata[1],\n",
    "                    frame_metadata=dataset.export_frame_metadata[1],\n",
    "                    labels=None,#nuclear_tracking.reordered_labels,\n",
    "                    expand_distance=3,\n",
    "                    search_range_um=4.2,\n",
    "                    retrack_search_range_um=4.5,\n",
    "                    threshold_factor=1.3,\n",
    "                    memory=3,\n",
    "                    retrack_after_filter=False,\n",
    "                    stitch=True,\n",
    "                    min_track_length=0,\n",
    "                    series_splits=dataset.series_splits,\n",
    "                    series_shifts=dataset.series_shifts,\n",
    "                    keep_bandpass=False,\n",
    "                    keep_futures=False,\n",
    "                    keep_spot_labels=False,\n",
    "                    evaluate=True,\n",
    "                    retrack_by_intensity=True,\n",
    "                    client=client,\n",
    "                )\n",
    "                \n",
    "                spot_tracking.extract_spot_traces(\n",
    "                    working_memory_folder=test_dataset_name, \n",
    "                    stitch=True,\n",
    "                    retrack_after_filter=True,\n",
    "                    trackpy_log_path = test_dataset_name+'/trackpy_log'\n",
    "                )\n",
    "                \n",
    "                # Saves tracked spot mask as a zarr, and pickles dataframes with spot fitting and\n",
    "                # quantification information.\n",
    "                spot_tracking.save_results(name_folder=test_dataset_name, save_array_as=None)\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "\n",
    "    print('\\n')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process data for: /mnt/Data1/Nick/transcription_pipeline/test_data/2025-03-18/MCP-mSG_His-RFP_RBSPWM(003)_embryo03\n",
      "Import MS2 from previous zarr files: True\n",
      "Import full embryo from previous zarr files: True\n",
      "Load from previous nuclear tracking results\n",
      "Load from spot tracking results\n",
      "\n",
      "\n",
      "Process data for: /mnt/Data1/Nick/transcription_pipeline/test_data/2025-03-18/MCP-mSG_His-RFP_RBSPWM(003)_embryo04\n",
      "Import MS2 from previous zarr files: True\n",
      "Import full embryo from previous zarr files: False\n",
      "Error: list index out of range\n",
      "Do nuclear tracking for the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/spot_pipeline.py:1241: UserWarning: `spot_dataframe` not found, keeping as `None`.\n",
      "  warnings.warn(\"`spot_dataframe` not found, keeping as `None`.\")\n",
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/nuclear_pipeline.py:60: UserWarning: Resolution is anisotropic in X and Y, segmentation parameters should be handled manually.\n",
      "  warnings.warn(\n",
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/nuclear_analysis/segmentation.py:135: UserWarning: No plateau found with specified max_differences, defaulting to minimizing successive differences.\n",
      "  warnings.warn(\n",
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/nuclear_analysis/segmentation.py:135: UserWarning: No plateau found with specified max_differences, defaulting to minimizing successive differences.\n",
      "  warnings.warn(\n",
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/nuclear_analysis/segmentation.py:135: UserWarning: No plateau found with specified max_differences, defaulting to minimizing successive differences.\n",
      "  warnings.warn(\n",
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/nuclear_analysis/segmentation.py:135: UserWarning: No plateau found with specified max_differences, defaulting to minimizing successive differences.\n",
      "  warnings.warn(\n",
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/nuclear_analysis/segmentation.py:135: UserWarning: No plateau found with specified max_differences, defaulting to minimizing successive differences.\n",
      "  warnings.warn(\n",
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/nuclear_analysis/segmentation.py:135: UserWarning: No plateau found with specified max_differences, defaulting to minimizing successive differences.\n",
      "  warnings.warn(\n",
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/nuclear_analysis/segmentation.py:135: UserWarning: No plateau found with specified max_differences, defaulting to minimizing successive differences.\n",
      "  warnings.warn(\n",
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/nuclear_analysis/segmentation.py:135: UserWarning: No plateau found with specified max_differences, defaulting to minimizing successive differences.\n",
      "  warnings.warn(\n",
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/nuclear_analysis/segmentation.py:135: UserWarning: No plateau found with specified max_differences, defaulting to minimizing successive differences.\n",
      "  warnings.warn(\n",
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/nuclear_analysis/segmentation.py:135: UserWarning: No plateau found with specified max_differences, defaulting to minimizing successive differences.\n",
      "  warnings.warn(\n",
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/nuclear_analysis/segmentation.py:135: UserWarning: No plateau found with specified max_differences, defaulting to minimizing successive differences.\n",
      "  warnings.warn(\n",
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/nuclear_analysis/segmentation.py:135: UserWarning: No plateau found with specified max_differences, defaulting to minimizing successive differences.\n",
      "  warnings.warn(\n",
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/nuclear_analysis/segmentation.py:135: UserWarning: No plateau found with specified max_differences, defaulting to minimizing successive differences.\n",
      "  warnings.warn(\n",
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/nuclear_analysis/segmentation.py:135: UserWarning: No plateau found with specified max_differences, defaulting to minimizing successive differences.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
