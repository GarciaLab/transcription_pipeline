{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to add\n",
    "\n",
    "### For `check_particle_fit`\n",
    "1. Change the unit to min instead of second\n",
    "2. Show all the traces instead of just the accepted ones\n",
    "3. In addition to the above, add a toggle parameter to look at only the accepted or rejected ones\n",
    "\n",
    "### For `check_bin_fit`\n",
    "1. Include approval status in the dataframe (Nick's suggestion: use a dictionary)\n",
    "\n",
    "### For both functions: add the linear fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset and Specify Parameters (please only edit cells in this section)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T00:21:53.093394Z",
     "start_time": "2025-02-11T00:21:53.090308Z"
    }
   },
   "source": [
    "# Parameters to specify\n",
    "\n",
    "# Specify here at what frame NC14 starts\n",
    "nc14_start_frame = 263\n",
    "\n",
    "# Any trace with frame number smaller than min_frames will be filtered out\n",
    "min_frames = 40\n",
    "\n",
    "# Time resolution (unit: second per frame)\n",
    "time_res_sec = 4.25939\n",
    "time_res_min = time_res_sec/60\n",
    "\n",
    "# Number of bins you want to split the full embryo into\n",
    "num_bins = 42"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T00:21:56.045200Z",
     "start_time": "2025-02-11T00:21:56.042697Z"
    }
   },
   "source": [
    "# Dataset Directory\n",
    "\n",
    "dataset_folder = '/mnt/Data1/Nick/transcription_pipeline/'\n",
    "\n",
    "RBSPWM_datasets = [\n",
    "    \"test_data/2024-02-26/Halo-RBSPWM_embryo01\",\n",
    "    \"test_data/2024-02-26/Halo-RBSPWM_embryo02\",\n",
    "    \"test_data/2024-05-07/Halo552-RBSPWM_embryo01\",\n",
    "    \"test_data/2024-05-07/Halo552-RBSPWM_embryo02\",\n",
    "    \"test_data/2024-05-09/Halo552-RBSPWM_embryo01\",\n",
    "]\n",
    "\n",
    "RBSVar2_datasets = [\n",
    "    \"test_data/2024-07-23/Halo673_RBSVar2_embryo01\",\n",
    "    \"test_data/2024-07-25/Halo673_RBSVar2_embryo01\", \n",
    "    \"test_data/2024-10-10/Halo673_RBSVar2_embryo01\",\n",
    "    \"test_data/2024-10-10/Halo673_RBSVar2_embryo02\",\n",
    "]\n",
    "\n",
    "MCP_mSG_datasets = [\n",
    "    \"test_data/2024-08-13/MCP-mSG,ParB-mScar_normWindow\",\n",
    "    \"test_data/2024-10-31/MCP-mSG_ParB-mScar_RBSPWM_embryo01\",\n",
    "    \"test_data/2024-10-31/MCP-mSG_ParB-mScar_RBSPWM_embryo02\",\n",
    "    \"test_data/2024-11-05/MCP-mSG_ParB-mScar_RBSPWM_embryo01\",\n",
    "    ]\n",
    "test_dataset_name = dataset_folder + RBSPWM_datasets[4]\n",
    "print('Dataset Path: ' + test_dataset_name)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Path: /mnt/Data1/Nick/transcription_pipeline/test_data/2024-05-09/Halo552-RBSPWM_embryo01\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-02-11T00:22:08.992076Z",
     "start_time": "2025-02-11T00:22:02.857647Z"
    }
   },
   "source": [
    "# Import pipeline\n",
    "from transcription_pipeline import nuclear_pipeline\n",
    "from transcription_pipeline import preprocessing_pipeline\n",
    "\n",
    "from transcription_pipeline import spot_pipeline\n",
    "from transcription_pipeline import fullEmbryo_pipeline\n",
    "\n",
    "# Importing libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "from transcription_pipeline.spot_analysis import compile_data\n",
    "from transcription_pipeline.utils import plottable\n",
    "\n",
    "from scipy.signal import medfilt\n",
    "from skimage.restoration import denoise_tv_chambolle\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import least_squares\n",
    "from scipy.stats import chi2\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import emcee\n",
    "import os\n",
    "from warnings import warn\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "from tkinter import messagebox\n",
    "\n",
    "root = tk.Tk()\n",
    "root.withdraw();"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`JAVA_HOME` environment variable set to /mnt/Data1/Nick/miniforge3/envs/transcription_pipeline\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T00:22:12.851865Z",
     "start_time": "2025-02-11T00:22:12.801559Z"
    }
   },
   "source": [
    "# Specify how you would want the plots to be shown: Use TkAgg if you use PyCharm, or widget if you use a browser\n",
    "\n",
    "mpl.use('TkAgg')\n",
    "# %matplotlib widget"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import MS2 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect whether the dataset has already been converted into `zarr` files, i.e. whether there's \"previously\" processed data. If so, load the previous results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T00:22:17.064857Z",
     "start_time": "2025-02-11T00:22:17.060391Z"
    }
   },
   "source": [
    "ms2_import_previous = os.path.isdir(test_dataset_name + '/collated_dataset')\n",
    "ms2_import_previous"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T00:22:19.384496Z",
     "start_time": "2025-02-11T00:22:19.366928Z"
    }
   },
   "source": [
    "dataset = preprocessing_pipeline.DataImport(\n",
    "    name_folder=test_dataset_name,\n",
    "    trim_series=True,\n",
    "    working_storage_mode='zarr',\n",
    "    import_previous=ms2_import_previous, \n",
    ")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T21:34:49.678823Z",
     "start_time": "2024-11-01T21:32:43.074715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = preprocessing_pipeline.DataImport(\n",
    "    name_folder=test_dataset_name,\n",
    "    trim_series=True,\n",
    "    working_storage_mode='zarr',\n",
    "    import_previous=False, \n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The series in ['Series002', 'Series003', 'Series004', 'Series005'] have inconsistent LaserID, check your imaging settings and metadata.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/preprocessing/import_data.py:863: UserWarning: Could not align z-stack after series 0.\n",
      "  warnings.warn(\n",
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/preprocessing/import_data.py:582: PeakPropertyWarning: some peaks have a prominence of 0\n",
      "  prominence = peak_prominences(offsets, [proposed_peak])[0]\n",
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/preprocessing/import_data.py:863: UserWarning: Could not align z-stack after series 1.\n",
      "  warnings.warn(\n",
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/preprocessing/import_data.py:863: UserWarning: Could not align z-stack after series 2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T21:49:55.347926Z",
     "start_time": "2024-11-01T21:49:55.343679Z"
    }
   },
   "cell_type": "code",
   "source": "dataset.export_frame_metadata[0]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frame': array([[   0,    1,    2, ...,   18,   19,   20],\n",
       "        [  21,   22,   23, ...,   39,   40,   41],\n",
       "        [  42,   43,   44, ...,   60,   61,   62],\n",
       "        ...,\n",
       "        [5208, 5209, 5210, ..., 5226, 5227, 5228],\n",
       "        [5229, 5230, 5231, ..., 5247, 5248, 5249],\n",
       "        [5250, 5251, 5252, ..., 5268, 5269, 5270]]),\n",
       " 'series': 0,\n",
       " 'mpp': 0.22194604105571847,\n",
       " 'mppZ': 0.4196171000000001,\n",
       " 'x': 0,\n",
       " 'y': 0,\n",
       " 'x_um': 0.0386673141867,\n",
       " 'y_um': -0.06760669878379,\n",
       " 'axes': ['t', 'c', 'z', 'y', 'x'],\n",
       " 'coords': {},\n",
       " 'c': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'colors': (0.0, 1.0, 0.0),\n",
       " 'z': array([[ 0,  1,  2, ..., 18, 19, 20],\n",
       "        [ 0,  1,  2, ..., 18, 19, 20],\n",
       "        [ 0,  1,  2, ..., 18, 19, 20],\n",
       "        ...,\n",
       "        [ 0,  1,  2, ..., 18, 19, 20],\n",
       "        [ 0,  1,  2, ..., 18, 19, 20],\n",
       "        [ 0,  1,  2, ..., 18, 19, 20]]),\n",
       " 't': array([[  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  1,   1,   1, ...,   1,   1,   1],\n",
       "        [  2,   2,   2, ...,   2,   2,   2],\n",
       "        ...,\n",
       "        [245, 245, 245, ..., 245, 245, 245],\n",
       "        [246, 246, 246, ..., 246, 246, 246],\n",
       "        [247, 247, 247, ..., 247, 247, 247]]),\n",
       " 't_s': array([[0.00000000e+00, 2.11999893e-01, 4.23000336e-01, ...,\n",
       "         3.80299950e+00, 4.01199913e+00, 4.22299957e+00],\n",
       "        [4.46800041e+00, 4.67499924e+00, 4.88299942e+00, ...,\n",
       "         8.26300049e+00, 8.47200012e+00, 8.68300056e+00],\n",
       "        [8.92300034e+00, 9.13199997e+00, 9.34300041e+00, ...,\n",
       "         1.27229996e+01, 1.29319992e+01, 1.31429996e+01],\n",
       "        ...,\n",
       "        [1.18237400e+03, 1.18258300e+03, 1.18280300e+03, ...,\n",
       "         1.18617100e+03, 1.18638300e+03, 1.18659100e+03],\n",
       "        [1.18684300e+03, 1.18705100e+03, 1.18726300e+03, ...,\n",
       "         1.19063100e+03, 1.19085100e+03, 1.19106300e+03],\n",
       "        [1.19130300e+03, 1.19151100e+03, 1.19172300e+03, ...,\n",
       "         1.19509100e+03, 1.19530300e+03, 1.19551100e+03]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T21:51:48.709863Z",
     "start_time": "2024-11-01T21:51:48.705955Z"
    }
   },
   "cell_type": "code",
   "source": "dataset.export_frame_metadata[0]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frame': array([[   0,    1,    2, ...,   18,   19,   20],\n",
       "        [  21,   22,   23, ...,   39,   40,   41],\n",
       "        [  42,   43,   44, ...,   60,   61,   62],\n",
       "        ...,\n",
       "        [5208, 5209, 5210, ..., 5226, 5227, 5228],\n",
       "        [5229, 5230, 5231, ..., 5247, 5248, 5249],\n",
       "        [5250, 5251, 5252, ..., 5268, 5269, 5270]]),\n",
       " 'series': 0,\n",
       " 'mpp': 0.22194604105571847,\n",
       " 'mppZ': 0.4196171000000001,\n",
       " 'x': 0,\n",
       " 'y': 0,\n",
       " 'x_um': 0.0386673141867,\n",
       " 'y_um': -0.06760669878379,\n",
       " 'axes': ['t', 'c', 'z', 'y', 'x'],\n",
       " 'coords': {},\n",
       " 'c': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'colors': (0.0, 1.0, 0.0),\n",
       " 'z': array([[ 0,  1,  2, ..., 18, 19, 20],\n",
       "        [ 0,  1,  2, ..., 18, 19, 20],\n",
       "        [ 0,  1,  2, ..., 18, 19, 20],\n",
       "        ...,\n",
       "        [ 0,  1,  2, ..., 18, 19, 20],\n",
       "        [ 0,  1,  2, ..., 18, 19, 20],\n",
       "        [ 0,  1,  2, ..., 18, 19, 20]]),\n",
       " 't': array([[  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  1,   1,   1, ...,   1,   1,   1],\n",
       "        [  2,   2,   2, ...,   2,   2,   2],\n",
       "        ...,\n",
       "        [245, 245, 245, ..., 245, 245, 245],\n",
       "        [246, 246, 246, ..., 246, 246, 246],\n",
       "        [247, 247, 247, ..., 247, 247, 247]]),\n",
       " 't_s': array([[0.00000000e+00, 2.11999893e-01, 4.23000336e-01, ...,\n",
       "         3.80299950e+00, 4.01199913e+00, 4.22299957e+00],\n",
       "        [4.46800041e+00, 4.67499924e+00, 4.88299942e+00, ...,\n",
       "         8.26300049e+00, 8.47200012e+00, 8.68300056e+00],\n",
       "        [8.92300034e+00, 9.13199997e+00, 9.34300041e+00, ...,\n",
       "         1.27229996e+01, 1.29319992e+01, 1.31429996e+01],\n",
       "        ...,\n",
       "        [1.18237400e+03, 1.18258300e+03, 1.18280300e+03, ...,\n",
       "         1.18617100e+03, 1.18638300e+03, 1.18659100e+03],\n",
       "        [1.18684300e+03, 1.18705100e+03, 1.18726300e+03, ...,\n",
       "         1.19063100e+03, 1.19085100e+03, 1.19106300e+03],\n",
       "        [1.19130300e+03, 1.19151100e+03, 1.19172300e+03, ...,\n",
       "         1.19509100e+03, 1.19530300e+03, 1.19551100e+03]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import FullEmbryo Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "FullEmbryo_dataset = preprocessing_pipeline.FullEmbryoImport(\n",
    "    name_folder=test_dataset_name,\n",
    "    #import_previous=True\n",
    ")\n",
    "# Loading FullEmbryo dataset is not working currently, but reported to Yovan where it only reads in the last channel\n",
    "# FullEmbryo_dataset.save()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting a DASK Client for parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-11-01T22:00:30.847487Z",
     "start_time": "2024-11-01T22:00:30.814437Z"
    }
   },
   "source": [
    "from dask.distributed import LocalCluster, Client\n",
    "\n",
    "try:\n",
    "    cluster = LocalCluster(\n",
    "        host=\"localhost\",\n",
    "        scheduler_port=37763,\n",
    "        threads_per_worker=1,\n",
    "        n_workers=14,\n",
    "        memory_limit=\"6GB\",\n",
    "    )\n",
    "    \n",
    "    client = Client(cluster)\n",
    "except:\n",
    "    print(\"Cluster already running\")\n",
    "    client = Client('localhost:37763')\n",
    "\n",
    "print(client)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Data1/Nick/miniforge3/envs/transcription_pipeline/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 37167 instead\n",
      "  warnings.warn(\n",
      "2024-11-01 15:00:30,840 - distributed.deploy.spec - WARNING - Cluster closed without starting up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster already running\n",
      "<Client: 'tcp://127.0.0.1:37763' processes=14 threads=14, memory=78.23 GiB>\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-11-01T21:52:13.163798Z",
     "start_time": "2024-11-01T21:52:12.792504Z"
    }
   },
   "source": "client.shutdown()",
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": "print(client.dashboard_link)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuclear Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect whether the nuclear tracking has been done \"previously.\" If so, load the previous results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "nuclear_tracking_previous = os.path.isdir(test_dataset_name + '/nuclear_analysis_results')\n",
    "nuclear_tracking_previous"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "if nuclear_tracking_previous:\n",
    "    # Load nuclear tracking results\n",
    "    print('Load from previous nuclear tracking results')\n",
    "    \n",
    "    nuclear_tracking = nuclear_pipeline.Nuclear()\n",
    "    nuclear_tracking.read_results(name_folder=test_dataset_name)\n",
    "    \n",
    "else:\n",
    "    # Do nuclear tracking and save the results\n",
    "    print('Do nuclear tracking for the dataset')\n",
    "    \n",
    "    nuclear_tracking = nuclear_pipeline.Nuclear(\n",
    "        data=dataset.channels_full_dataset[0],\n",
    "        global_metadata=dataset.export_global_metadata[0],\n",
    "        frame_metadata=dataset.export_frame_metadata[0],\n",
    "        series_splits=dataset.series_splits,\n",
    "        series_shifts=dataset.series_shifts,\n",
    "        search_range_um=1.5,\n",
    "        stitch=False,\n",
    "        stitch_max_distance=4,\n",
    "        stitch_max_frame_distance=2,\n",
    "        client=client,\n",
    "        keep_futures=False,\n",
    "    )\n",
    "    \n",
    "    nuclear_tracking.track_nuclei(\n",
    "            working_memory_mode=\"zarr\",\n",
    "            working_memory_folder=test_dataset_name,\n",
    "            trackpy_log_path=\"\".join([test_dataset_name, \"trackpy_log\"]),\n",
    "        )\n",
    "        # Saves tracked nuclear mask as a zarr, and pickles dataframes with segmentation and\n",
    "        # tracking information.\n",
    "    nuclear_tracking.save_results(\n",
    "            name_folder=test_dataset_name, save_array_as=None\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spot Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect whether the spot tracking has been done \"previously.\" If so, load the previous results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T00:22:40.261898Z",
     "start_time": "2025-02-11T00:22:40.258996Z"
    }
   },
   "source": [
    "spot_tracking_previous = os.path.isdir(test_dataset_name + '/spot_analysis_results')\n",
    "spot_tracking_previous"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-02-11T00:22:45.294487Z",
     "start_time": "2025-02-11T00:22:42.009738Z"
    }
   },
   "source": [
    "%%time\n",
    "\n",
    "if spot_tracking_previous:\n",
    "    # Load spot tracking results\n",
    "    print('Load from spot  tracking results')\n",
    "    \n",
    "    spot_tracking = spot_pipeline.Spot()\n",
    "    spot_tracking.read_results(name_folder=test_dataset_name)\n",
    "    \n",
    "else:\n",
    "    # Do spot tracking and save the results\n",
    "    print('Do spot tracking for the dataset')\n",
    "    \n",
    "    spot_tracking = spot_pipeline.Spot(\n",
    "        data=dataset.channels_full_dataset[1],\n",
    "        global_metadata=dataset.export_global_metadata[1],\n",
    "        frame_metadata=dataset.export_frame_metadata[1],\n",
    "        labels=None,#nuclear_tracking.reordered_labels,\n",
    "        expand_distance=3,\n",
    "        search_range_um=4.2,\n",
    "        retrack_search_range_um=4.5,\n",
    "        threshold_factor=1.3,\n",
    "        memory=3,\n",
    "        retrack_after_filter=False,\n",
    "        stitch=True,\n",
    "        min_track_length=0,\n",
    "        series_splits=dataset.series_splits,\n",
    "        series_shifts=dataset.series_shifts,\n",
    "        keep_bandpass=False,\n",
    "        keep_futures=False,\n",
    "        keep_spot_labels=False,\n",
    "        evaluate=True,\n",
    "        retrack_by_intensity=True,\n",
    "        client=client,\n",
    "    )\n",
    "    \n",
    "    spot_tracking.extract_spot_traces(\n",
    "        working_memory_folder=test_dataset_name, \n",
    "        stitch=True,\n",
    "        retrack_after_filter=True,\n",
    "        trackpy_log_path = test_dataset_name+'/trackpy_log'\n",
    "    )\n",
    "    \n",
    "    # Saves tracked spot mask as a zarr, and pickles dataframes with spot fitting and\n",
    "    # quantification information.\n",
    "    spot_tracking.save_results(name_folder=test_dataset_name, save_array_as=None)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load from spot  tracking results\n",
      "CPU times: user 1.35 s, sys: 412 ms, total: 1.76 s\n",
      "Wall time: 3.28 s\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T21:56:04.515113Z",
     "start_time": "2024-11-01T21:52:24.730141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    spot_tracking = spot_pipeline.Spot(\n",
    "        data=dataset.channels_full_dataset[1],\n",
    "        global_metadata=dataset.export_global_metadata[1],\n",
    "        frame_metadata=dataset.export_frame_metadata[1],\n",
    "        labels=None,#nuclear_tracking.reordered_labels,\n",
    "        expand_distance=3,\n",
    "        search_range_um=4.2,\n",
    "        retrack_search_range_um=4.5,\n",
    "        threshold_factor=1.3,\n",
    "        memory=0,\n",
    "        retrack_after_filter=False,\n",
    "        stitch=False,\n",
    "        min_track_length=0,\n",
    "        series_splits=dataset.series_splits,\n",
    "        series_shifts=dataset.series_shifts,\n",
    "        keep_bandpass=False,\n",
    "        keep_futures=False,\n",
    "        keep_spot_labels=False,\n",
    "        evaluate=True,\n",
    "        retrack_by_intensity=True,\n",
    "        client=client,\n",
    "    )\n",
    "    \n",
    "    spot_tracking.extract_spot_traces(\n",
    "        working_memory_folder=test_dataset_name, \n",
    "        stitch=True,\n",
    "        retrack_after_filter=True,\n",
    "        trackpy_log_path = test_dataset_name+'/trackpy_log'\n",
    "    )\n",
    "    \n",
    "    # Saves tracked spot mask as a zarr, and pickles dataframes with spot fitting and\n",
    "    # quantification information.\n",
    "    spot_tracking.save_results(name_folder=test_dataset_name, save_array_as=None)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/spot_pipeline.py:202: UserWarning: Resolution is anisotropic in X and Y, segmentation parameters should be handled manually.\n",
      "  warnings.warn(\n",
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/spot_pipeline.py:722: UserWarning: `working_memory_mode` set to 'zarr', will not pull `spot_labels` into memory.\n",
      "  warnings.warn(\n",
      "/mnt/Data1/Nick/miniforge3/envs/transcription_pipeline/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/mnt/Data1/Nick/miniforge3/envs/transcription_pipeline/lib/python3.10/site-packages/numpy/core/_methods.py:121: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "/mnt/Data1/Nick/miniforge3/envs/transcription_pipeline/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/mnt/Data1/Nick/miniforge3/envs/transcription_pipeline/lib/python3.10/site-packages/numpy/core/_methods.py:121: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "/mnt/Data1/Nick/miniforge3/envs/transcription_pipeline/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/mnt/Data1/Nick/miniforge3/envs/transcription_pipeline/lib/python3.10/site-packages/numpy/core/_methods.py:121: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "/mnt/Data1/Nick/miniforge3/envs/transcription_pipeline/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/mnt/Data1/Nick/miniforge3/envs/transcription_pipeline/lib/python3.10/site-packages/numpy/core/_methods.py:121: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "/mnt/Data1/Nick/miniforge3/envs/transcription_pipeline/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/mnt/Data1/Nick/miniforge3/envs/transcription_pipeline/lib/python3.10/site-packages/numpy/core/_methods.py:121: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "/mnt/Data1/Nick/miniforge3/envs/transcription_pipeline/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/mnt/Data1/Nick/miniforge3/envs/transcription_pipeline/lib/python3.10/site-packages/numpy/core/_methods.py:121: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "/mnt/Data1/Nick/miniforge3/envs/transcription_pipeline/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/mnt/Data1/Nick/miniforge3/envs/transcription_pipeline/lib/python3.10/site-packages/numpy/core/_methods.py:121: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "/mnt/Data1/Nick/miniforge3/envs/transcription_pipeline/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/mnt/Data1/Nick/miniforge3/envs/transcription_pipeline/lib/python3.10/site-packages/numpy/core/_methods.py:121: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "/mnt/Data1/Nick/miniforge3/envs/transcription_pipeline/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/mnt/Data1/Nick/miniforge3/envs/transcription_pipeline/lib/python3.10/site-packages/numpy/core/_methods.py:121: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "/mnt/Data1/Nick/miniforge3/envs/transcription_pipeline/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/mnt/Data1/Nick/miniforge3/envs/transcription_pipeline/lib/python3.10/site-packages/numpy/core/_methods.py:121: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking and filtering:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preliminary spot filtering: 100%|██████████| 2/2 [00:00<00:00, 129.51it/s]\n",
      "Tracking: 100%|██████████| 247/247 [00:03<00:00, 73.09it/s] \n",
      "Post-tracking spot filtering: 100%|██████████| 2/2 [00:00<00:00, 59.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-tracking after filtering:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preliminary spot filtering: 100%|██████████| 2/2 [00:00<00:00, 627.65it/s]\n",
      "Compiling variations in intensity: 100%|██████████| 9893/9893 [00:02<00:00, 4757.86it/s]\n",
      "Tracking: 100%|██████████| 247/247 [00:01<00:00, 171.62it/s]\n",
      "Post-tracking spot filtering: 100%|██████████| 2/2 [00:00<00:00, 280.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stitching pass 1 of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding track y start position: 100%|██████████| 566/566 [00:00<00:00, 4132.50it/s]\n",
      "Finding track y end position: 100%|██████████| 566/566 [00:00<00:00, 4156.16it/s]\n",
      "Finding track x start position: 100%|██████████| 566/566 [00:00<00:00, 4342.23it/s]\n",
      "Finding track x end position: 100%|██████████| 566/566 [00:00<00:00, 3923.24it/s]\n",
      "Finding track first and last frames: 100%|██████████| 566/566 [00:00<00:00, 6604.49it/s]\n",
      "Stitching track nearest neighbors: 100%|██████████| 566/566 [00:02<00:00, 263.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stitching tracks: 100%|██████████| 28/28 [00:00<00:00, 81.66it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing duplicate spots from stitching: 100%|██████████| 12338/12338 [00:02<00:00, 5523.10it/s]\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Compiled Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T00:22:56.823091Z",
     "start_time": "2025-02-11T00:22:54.575007Z"
    }
   },
   "source": [
    "# Load spot tracking dataframe\n",
    "spot_df = spot_tracking.spot_dataframe\n",
    "\n",
    "# Remove spots that were not detected\n",
    "detected_spots = spot_df[spot_df[\"particle\"] != 0]\n",
    "\n",
    "# Compile traces\n",
    "compiled_dataframe = compile_data.compile_traces(\n",
    "    detected_spots,\n",
    "    compile_columns_spot=[\n",
    "        \"frame\",\n",
    "        \"t_s\",\n",
    "        \"intensity_from_neighborhood\",\n",
    "        \"intensity_std_error_from_neighborhood\",\n",
    "        \"x\",\n",
    "        \"y\"\n",
    "    ],\n",
    "    nuclear_tracking_dataframe=None,\n",
    ")\n",
    "\n",
    "compiled_dataframe.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   particle                                              frame  \\\n",
       "0         2  [608, 609, 610, 613, 614, 615, 616, 617, 618, ...   \n",
       "1         3  [579, 580, 581, 582, 583, 584, 585, 586, 587, ...   \n",
       "2         4  [640, 641, 642, 643, 644, 645, 647, 648, 649, ...   \n",
       "3         5  [613, 614, 615, 616, 617, 618, 619, 620, 621, ...   \n",
       "4         6  [583, 585, 586, 588, 590, 591, 592, 593, 594, ...   \n",
       "\n",
       "                                                 t_s  \\\n",
       "0  [2981.271999359131, 2985.51900100708, 2989.767...   \n",
       "1  [2857.3950004577637, 2861.64400100708, 2865.89...   \n",
       "2  [3117.5319995880127, 3121.579999923706, 3126.1...   \n",
       "3  [3002.3190002441406, 3006.1590003967285, 3010....   \n",
       "4  [2874.9950008392334, 2883.492000579834, 2887.7...   \n",
       "\n",
       "                         intensity_from_neighborhood  \\\n",
       "0  [67.20298245614036, 62.61358125, 92.1778863636...   \n",
       "1  [368.71108000000004, 162.58609316770185, 207.0...   \n",
       "2  [137.8392151898734, 246.30097297297297, 162.72...   \n",
       "3  [223.98077931034484, 149.44997701149424, 102.1...   \n",
       "4  [56.80923595505618, 58.452866666666665, 56.477...   \n",
       "\n",
       "               intensity_std_error_from_neighborhood  \\\n",
       "0  [54.07608080882359, 53.08599141703332, 48.6846...   \n",
       "1  [48.961304741944936, 48.764177034702456, 49.24...   \n",
       "2  [50.04719298970737, 48.033152885448544, 53.299...   \n",
       "3  [52.019684703514685, 46.46238563874822, 49.753...   \n",
       "4  [45.10765132358713, 45.86984351271528, 48.0794...   \n",
       "\n",
       "                                                   x  \\\n",
       "0  [706.3077781092006, 705.0800225329217, 704.078...   \n",
       "1  [814.23756413008, 814.0490753702337, 812.66835...   \n",
       "2  [610.5256101215496, 610.5078280423238, 610.170...   \n",
       "3  [789.4493998034413, 790.1299534943532, 790.754...   \n",
       "4  [830.0205022942665, 828.8195400854084, 827.835...   \n",
       "\n",
       "                                                   y  \n",
       "0  [132.22746686990476, 133.33576781054558, 131.9...  \n",
       "1  [165.32374456695854, 165.4022049366289, 164.15...  \n",
       "2  [200.09244408786026, 199.6949788757095, 200.19...  \n",
       "3  [111.42483044437226, 110.54379569293525, 110.3...  \n",
       "4  [152.07172254602511, 153.15739986339034, 152.5...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>particle</th>\n",
       "      <th>frame</th>\n",
       "      <th>t_s</th>\n",
       "      <th>intensity_from_neighborhood</th>\n",
       "      <th>intensity_std_error_from_neighborhood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>[608, 609, 610, 613, 614, 615, 616, 617, 618, ...</td>\n",
       "      <td>[2981.271999359131, 2985.51900100708, 2989.767...</td>\n",
       "      <td>[67.20298245614036, 62.61358125, 92.1778863636...</td>\n",
       "      <td>[54.07608080882359, 53.08599141703332, 48.6846...</td>\n",
       "      <td>[706.3077781092006, 705.0800225329217, 704.078...</td>\n",
       "      <td>[132.22746686990476, 133.33576781054558, 131.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>[579, 580, 581, 582, 583, 584, 585, 586, 587, ...</td>\n",
       "      <td>[2857.3950004577637, 2861.64400100708, 2865.89...</td>\n",
       "      <td>[368.71108000000004, 162.58609316770185, 207.0...</td>\n",
       "      <td>[48.961304741944936, 48.764177034702456, 49.24...</td>\n",
       "      <td>[814.23756413008, 814.0490753702337, 812.66835...</td>\n",
       "      <td>[165.32374456695854, 165.4022049366289, 164.15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>[640, 641, 642, 643, 644, 645, 647, 648, 649, ...</td>\n",
       "      <td>[3117.5319995880127, 3121.579999923706, 3126.1...</td>\n",
       "      <td>[137.8392151898734, 246.30097297297297, 162.72...</td>\n",
       "      <td>[50.04719298970737, 48.033152885448544, 53.299...</td>\n",
       "      <td>[610.5256101215496, 610.5078280423238, 610.170...</td>\n",
       "      <td>[200.09244408786026, 199.6949788757095, 200.19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>[613, 614, 615, 616, 617, 618, 619, 620, 621, ...</td>\n",
       "      <td>[3002.3190002441406, 3006.1590003967285, 3010....</td>\n",
       "      <td>[223.98077931034484, 149.44997701149424, 102.1...</td>\n",
       "      <td>[52.019684703514685, 46.46238563874822, 49.753...</td>\n",
       "      <td>[789.4493998034413, 790.1299534943532, 790.754...</td>\n",
       "      <td>[111.42483044437226, 110.54379569293525, 110.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>[583, 585, 586, 588, 590, 591, 592, 593, 594, ...</td>\n",
       "      <td>[2874.9950008392334, 2883.492000579834, 2887.7...</td>\n",
       "      <td>[56.80923595505618, 58.452866666666665, 56.477...</td>\n",
       "      <td>[45.10765132358713, 45.86984351271528, 48.0794...</td>\n",
       "      <td>[830.0205022942665, 828.8195400854084, 827.835...</td>\n",
       "      <td>[152.07172254602511, 153.15739986339034, 152.5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Embryo Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(FullEmbryo_dataset.channels_full_dataset_surf[0][0, :, :], cmap='gray')\n",
    "plt.title('Full Embryo Surf')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(FullEmbryo_dataset.channels_full_dataset_mid[0][0, :, :], cmap='gray')\n",
    "plt.title('Full Embryo Mid')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fullEmbryo = fullEmbryo_pipeline.FullEmbryo(FullEmbryo_dataset, dataset, his_channel=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fullEmbryo.find_ap_axis(make_plots=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "compiled_dataframe = fullEmbryo.xy_to_ap(compiled_dataframe)\n",
    "compiled_dataframe.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Individual Traces as a Check"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T00:23:02.028414Z",
     "start_time": "2025-02-11T00:23:02.024468Z"
    }
   },
   "source": [
    "# Restrict to longer traces\n",
    "traces_compiled_dataframe = compiled_dataframe[\n",
    "    compiled_dataframe[\"frame\"].apply(lambda x: x.size) > min_frames\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-02-11T00:25:06.945267Z",
     "start_time": "2025-02-11T00:23:04.560797Z"
    }
   },
   "source": [
    "# Copied from Nick's Analysis Notebook\n",
    "\n",
    "# The part of the code for scrolling between plots is taken from https://stackoverflow.com/questions/18390461/scroll-backwards-and-forwards-through-matplotlib-plots\n",
    "\n",
    "traces = plottable.generate_trace_plot_list(traces_compiled_dataframe)\n",
    "#median_filtered_traces = [medfilt(trace[1], kernel_size=15) for trace in traces]\n",
    "\n",
    "tv_denoised_traces = [\n",
    "    denoise_tv_chambolle(trace[1], weight=1080, max_num_iter=500) for trace in traces\n",
    "]\n",
    "# potts_steps_traces = [\n",
    "#     potts_l1.l1_potts_step_detection(trace[1], gamma=-5e3, weights=(1 / trace[2] ** 2))\n",
    "#     for trace in traces\n",
    "# ]\n",
    "\n",
    "\n",
    "# plt.close()\n",
    "# plt.plot(traces[curr_pos][0], traces[curr_pos][1], label=\"Original\")otts_l1.l1_potts_step_detection(trace[1], gamma=-5e3, weights=(1 / trace[2] ** 2))\n",
    "#     for trace in traces\n",
    "# ]\n",
    "\n",
    "curr_pos = 0\n",
    "\n",
    "\n",
    "def key_event(e):\n",
    "    global curr_pos\n",
    "\n",
    "    if e.key == \"right\":\n",
    "        curr_pos = curr_pos + 1\n",
    "    elif e.key == \"left\":\n",
    "        curr_pos = curr_pos - 1\n",
    "    else:\n",
    "        return\n",
    "    curr_pos = curr_pos % len(traces)\n",
    "\n",
    "    ax.cla()\n",
    "    ax.errorbar(\n",
    "        traces[curr_pos][0],\n",
    "        traces[curr_pos][1],\n",
    "        yerr=traces[curr_pos][2],\n",
    "        fmt=\".\",\n",
    "        elinewidth=1,\n",
    "    )\n",
    "    # ax.plot(traces[curr_pos][0], median_filtered_traces[curr_pos], color=\"k\")\n",
    "    ax.plot(traces[curr_pos][0], tv_denoised_traces[curr_pos], color=\"k\", label=\"TV\")\n",
    "    # ax.step(\n",
    "    #     traces[curr_pos][0],\n",
    "    #     potts_steps_traces[curr_pos],\n",
    "    #     where=\"mid\",\n",
    "    #     color=\"red\",\n",
    "    #     label=\"Potts L1\",\n",
    "    # )\n",
    "    ax.set_xlabel(\"time (s)\")\n",
    "    ax.set_ylabel(\"Spot intensity (AU)\")\n",
    "\n",
    "    particle = traces[curr_pos][3]\n",
    "    mean_x = (\n",
    "        compiled_dataframe.loc[compiled_dataframe[\"particle\"] == particle, \"x\"]\n",
    "        .values[0]\n",
    "        .mean()\n",
    "    )\n",
    "    initial_frame = (\n",
    "    compiled_dataframe.loc[compiled_dataframe[\"particle\"] == particle, \"frame\"]\n",
    "    .values[0][0]\n",
    "    )\n",
    "    ax.set_title(f\"Particle {particle}, x = {mean_x}, Initial frame {initial_frame}\")\n",
    "    ax.legend()\n",
    "    fig.canvas.draw()\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.canvas.mpl_connect(\"key_press_event\", key_event)\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "ax.errorbar(\n",
    "    traces[curr_pos][0],\n",
    "    traces[curr_pos][1],\n",
    "    yerr=traces[curr_pos][2],\n",
    "    fmt=\".\",\n",
    "    elinewidth=1,\n",
    ")\n",
    "# ax.plot(traces[curr_pos][0], median_filtered_traces[curr_pos], color=\"k\")\n",
    "ax.plot(traces[curr_pos][0], tv_denoised_traces[curr_pos], color=\"k\", label=\"TV\")\n",
    "# ax.step(\n",
    "#     traces[curr_pos][0],\n",
    "#     potts_steps_traces[curr_pos],\n",
    "#     where=\"mid\",\n",
    "#     color=\"red\",\n",
    "#     label=\"Potts L1\",\n",
    "# )\n",
    "ax.set_xlabel(\"time (s)\")\n",
    "ax.set_ylabel(\"Spot intensity (AU)\")\n",
    "\n",
    "particle = traces[curr_pos][3]\n",
    "mean_x = (\n",
    "    compiled_dataframe.loc[compiled_dataframe[\"particle\"] == particle, \"x\"]\n",
    "    .values[0]\n",
    "    .mean()\n",
    ")\n",
    "initial_frame = (\n",
    "    compiled_dataframe.loc[compiled_dataframe[\"particle\"] == particle, \"frame\"]\n",
    "    .values[0][0]\n",
    ")\n",
    "ax.set_title(f\"Particle {particle}, x = {mean_x}, Initial frame {initial_frame}\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/utils/plottable.py:17: UserWarning: Could not determine division time, using absolute time.\n",
      "  warnings.warn(\"Could not determine division time, using absolute time.\")\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheme 1 (Fit & Average): Fitting to Individual MS2 Traces and then Take Their Average for Each Bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Fit Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T21:45:27.846252Z",
     "start_time": "2025-02-13T21:45:27.835978Z"
    }
   },
   "source": [
    "# fit_all_traces: a function that generates the fit for each trace\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import least_squares\n",
    "from scipy.stats import chi2\n",
    "import pandas as pd\n",
    "import emcee\n",
    "\n",
    "# Version with normalization and regularization\n",
    "def make_half_cycle(basal, t_on, t_dwell, rate, t_interp):\n",
    "    half_cycle = np.zeros_like(t_interp)\n",
    "    half_cycle[t_interp < t_on] = basal\n",
    "    half_cycle[(t_interp >= t_on) & (t_interp < t_on + t_dwell)] = basal + rate * (t_interp[(t_interp >= t_on) & (t_interp < t_on + t_dwell)] - t_on)\n",
    "    half_cycle[t_interp >= t_on + t_dwell] = basal + rate * t_dwell\n",
    "    return half_cycle\n",
    "\n",
    "def fit_func(params, MS2, timepoints, t_interp):\n",
    "    return np.interp(timepoints, t_interp, make_half_cycle(*params, t_interp)) - MS2\n",
    "\n",
    "def initial_guess(MS2, timepoints):\n",
    "    # Initial guess for the parameters\n",
    "    basal0 = MS2[0]\n",
    "    t_on0 = timepoints[0]\n",
    "    t_dwell0 = (2/3)*(timepoints[-1]-timepoints[0])\n",
    "    rate0 = 1\n",
    "    # print(np.max(mean_dy_dx))\n",
    "    return [basal0, t_on0, t_dwell0, rate0]\n",
    "\n",
    "\n",
    "def fit_half_cycle(MS2, timepoints, t_interp, std_errors, max_nfev=3000):\n",
    "    # Initial guess\n",
    "    x0 = initial_guess(MS2, timepoints)\n",
    "    \n",
    "    # Parameter bounds\n",
    "    lb = [np.min(MS2), 0, 0, 0]  # Ensure t_dwell is non-negative\n",
    "    ub = [np.max(MS2), np.max(timepoints), np.max(timepoints), 1e7]\n",
    "\n",
    "    # Scaling factors to normalize parameters\n",
    "    scale_factors = np.array([np.max(MS2), np.max(timepoints), np.max(timepoints), 100])\n",
    "\n",
    "    # Scaled bounds\n",
    "    lb_scaled = np.array(lb) / scale_factors\n",
    "    ub_scaled = np.array(ub) / scale_factors\n",
    "    x0_scaled = np.array(x0) / scale_factors\n",
    "\n",
    "    # Scaled fit function\n",
    "    def fit_func_scaled(params, MS2, timepoints, t_interp):\n",
    "        params_unscaled = params * scale_factors\n",
    "        return fit_func(params_unscaled, MS2, timepoints, t_interp)\n",
    "\n",
    "    # Negative log-likelihood function\n",
    "    def negative_log_likelihood(params, MS2, timepoints, t_interp, std_errors, reg=1e-3):\n",
    "        residuals = fit_func_scaled(params, MS2, timepoints, t_interp) / std_errors\n",
    "        regularization = reg * np.sum(params[:]**2)\n",
    "        nll = 0.5 * np.sum(residuals**2) + regularization\n",
    "        return nll\n",
    "\n",
    "    # Initial parameter estimation using least_squares\n",
    "    res = least_squares(negative_log_likelihood, \n",
    "                        x0_scaled, bounds=(lb_scaled, ub_scaled), \n",
    "                        args=(MS2, timepoints, t_interp, std_errors), max_nfev=max_nfev)\n",
    "    \n",
    "\n",
    "    # Define log-probability function for MCMC\n",
    "    def log_prob(params, MS2, timepoints, t_interp, std_errors, scale_factors, lb_scaled, ub_scaled):\n",
    "        if np.any(params < lb_scaled) or np.any(params > ub_scaled):\n",
    "            return -np.inf\n",
    "        nll = negative_log_likelihood(params, MS2, timepoints, t_interp, std_errors)\n",
    "        return -nll  # Convert to log-probability\n",
    "\n",
    "    # MCMC parameters\n",
    "    nwalkers = 10\n",
    "    ndim = len(x0_scaled)\n",
    "    nsteps = 1000\n",
    "    initial_pos = res.x + 1e-4 * np.random.randn(nwalkers, ndim)\n",
    "    # Run MCMC\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, log_prob, args=(MS2, timepoints,\n",
    "                                                                    t_interp, std_errors,\n",
    "                                                                    scale_factors, lb_scaled, ub_scaled))\n",
    "    # Run MCMC until the acceptance fraction is at least 0.5\n",
    "    sampler.run_mcmc(initial_pos, nsteps, \n",
    "                     progress=False, tune=True)\n",
    "\n",
    "    # Flatten the chain and discard burn-in steps\n",
    "    flat_samples = sampler.get_chain(discard=200, thin=15, flat=True)\n",
    "\n",
    "    # Extract and rescale fit parameters\n",
    "    basal, t_on, t_dwell, rate = np.median(flat_samples, axis=0) * scale_factors\n",
    "\n",
    "    # Calculate confidence intervals\n",
    "    CI = np.percentile(flat_samples, [5, 95], axis=0).T * scale_factors[:, np.newaxis]\n",
    "\n",
    "    return basal, t_on, t_dwell, rate, CI\n",
    "\n",
    "def first_derivative(x, y):\n",
    "    \"\"\"\n",
    "    Compute the first discrete derivative of y with respect to x.\n",
    "    Parameters:\n",
    "    x (numpy.ndarray): Independent variable data points.\n",
    "    y (numpy.ndarray): Dependent variable data points.\n",
    "    Returns:\n",
    "    numpy.ndarray: Discrete first derivative of y with respect to x.\n",
    "    \"\"\"\n",
    "    dx = np.diff(x)\n",
    "    dy = np.diff(y)\n",
    "    dydx = dy / dx\n",
    "\n",
    "    # Use central differences for the interior points and forward/backward differences for the endpoints\n",
    "    dydx_central = np.zeros_like(y)\n",
    "    dydx_central[1:-1] = (y[2:] - y[:-2]) / (x[2:] - x[:-2])\n",
    "    dydx_central[0] = dydx[0]\n",
    "    dydx_central[-1] = dydx[-1]\n",
    "\n",
    "    return dydx_central\n",
    "\n",
    "def mean_sign_intervals(function):\n",
    "    \"\"\"\n",
    "    Compute the mean of function over intervals where the function has a constant sign.\n",
    "    Parameters:\n",
    "    derivative (numpy.ndarray): Array representing the function.\n",
    "    Returns:\n",
    "    numpy.ndarray: Array with mean values of the function over intervals with constant sign.\n",
    "    \"\"\"\n",
    "    # Identify where the sign changes\n",
    "    sign_changes = np.diff(np.sign(function))\n",
    "    # Get indices where the sign changes\n",
    "    change_indices = np.where(sign_changes != 0)[0] + 1\n",
    "\n",
    "    # Initialize the list to hold mean values\n",
    "    mean_values = []\n",
    "    start_index = 0\n",
    "\n",
    "    for end_index in change_indices:\n",
    "        # Calculate the mean of the current interval\n",
    "        interval_mean = np.mean(function[start_index:end_index])\n",
    "        # Append the mean value to the list\n",
    "        mean_values.extend([interval_mean] * (end_index - start_index))\n",
    "        # Update the start index\n",
    "        start_index = end_index\n",
    "\n",
    "    # Handle the last interval\n",
    "    interval_mean = np.mean(function[start_index:])\n",
    "    mean_values.extend([interval_mean] * (len(function) - start_index))\n",
    "\n",
    "    return np.array(mean_values), change_indices\n",
    "\n",
    "# Function to generate fits for all traces\n",
    "def fit_all_traces(traces, tv_denoised_traces):\n",
    "    \"\"\"\n",
    "    Fit half-cycles to all traces in the dataset.\n",
    "    Parameters:\n",
    "    traces (list): List of traces to fit.\n",
    "    tv_denoised_traces (list): List of TV denoised traces.\n",
    "    Returns:\n",
    "    list: List of tuples with fit parameters for each trace.\n",
    "    \"\"\"\n",
    "    # Initialize the list to hold fit results\n",
    "    fit_results = []\n",
    "\n",
    "    # Create new dataframe to store fit results\n",
    "    dataframe = pd.DataFrame(columns=['particle', 'fit_results'])\n",
    "    \n",
    "    for i in range(len(traces)):\n",
    "        # Compute the first derivative of TV denoised with respect to time\n",
    "        dy_dx = first_derivative(traces[i][0], tv_denoised_traces[i])\n",
    "\n",
    "        # Compute the mean of the first derivative over intervals with constant sign\n",
    "        mean_dy_dx, change_indices = mean_sign_intervals(dy_dx)\n",
    "\n",
    "        # Keep datapoints from before first sign change\n",
    "        try:\n",
    "            timepoints = traces[i][0][:change_indices[0]]\n",
    "            MS2 = traces[i][1][:change_indices[0]]\n",
    "            MS2_std = traces[i][2][:change_indices[0]]\n",
    "\n",
    "            # Interpolate the timepoints\n",
    "            t_interp = np.linspace(min(timepoints), max(timepoints), 1000)\n",
    "        except:\n",
    "            print(f\"Failed to find derivative sign change for trace {traces[i][3]}\")\n",
    "            fit_results.append([None, None, None, None, None])\n",
    "            dataframe.loc[i] = [traces[i][3], [None, None, None, None, None]]\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Compute the fit values\n",
    "        try:\n",
    "            basal, t_on, t_dwell, rate, CI = fit_half_cycle(MS2, timepoints, t_interp, MS2_std)\n",
    "            fit_result = [timepoints, t_interp, MS2, make_half_cycle(basal, t_on, t_dwell, rate, t_interp),\n",
    "                                [basal, t_on, t_dwell, rate, CI]]\n",
    "            \n",
    "            fit_results.append(fit_result)\n",
    "            dataframe.loc[i] = [traces[i][3], fit_result]\n",
    "        except:\n",
    "            print(f\"Failed to fit trace {traces[i][3]}\")\n",
    "            fit_results.append([timepoints,t_interp, MS2, None, None])\n",
    "            dataframe.loc[i] = [traces[i][3], [timepoints,t_interp, MS2, None, None]]\n",
    "            continue\n",
    "\n",
    "    return fit_results, dataframe"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Fitting on Ordered Spots"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "source": [
    "# Restrict to longer traces\n",
    "traces_compiled_dataframe = compiled_dataframe[\n",
    "    compiled_dataframe[\"frame\"].apply(lambda x: x.size) > min_frames\n",
    "]\n",
    "# Restrict to traces starting at frame nc14_start_frame and above\n",
    "traces_compiled_dataframe = traces_compiled_dataframe[\n",
    "    traces_compiled_dataframe[\"frame\"].apply(lambda x: x[0] >= nc14_start_frame)\n",
    "]\n",
    "\n",
    "# Order the traces based on the mean x position\n",
    "traces_compiled_dataframe = traces_compiled_dataframe.sort_values(\n",
    "    by=\"x\", key=lambda x: x.apply(np.mean)\n",
    ")\n",
    "\n",
    "traces = plottable.generate_trace_plot_list(traces_compiled_dataframe)\n",
    "\n",
    "\n",
    "# Generate TV denoised traces\n",
    "tv_denoised_traces = [\n",
    "    denoise_tv_chambolle(trace[1], weight=1080, max_num_iter=500) for trace in traces\n",
    "]\n",
    "\n",
    "# Generate fits for all traces\n",
    "fit_results, dataframe = fit_all_traces(traces, tv_denoised_traces)\n",
    "\n",
    "print(f\"Number of traces: {len(traces)}\")\n",
    "\n",
    "# Show number of traces with valid fits\n",
    "print(f\"Number of traces with valid fits: {sum([result[4] is not None for result in fit_results])}\")\n",
    "\n",
    "# Show number of traces with invalid fits\n",
    "print(f\"Number of traces with invalid fits: {sum([result[4] is None for result in fit_results])}\")\n",
    "\n",
    "traces_compiled_dataframe_fits = pd.merge(traces_compiled_dataframe, dataframe, on='particle', how='inner')\n",
    "\n",
    "# Add columns: the approval status, denoised trace, and fitted rate of the particle\n",
    "length = traces_compiled_dataframe_fits.index.max()\n",
    "status = [1 for _ in range(length+1)]\n",
    "traces_compiled_dataframe_fits['tv_denoised_trace'] = tv_denoised_traces\n",
    "traces_compiled_dataframe_fits['approval_status'] = status"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "traces_compiled_dataframe_fits.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the Fits"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T21:44:54.776775Z",
     "start_time": "2025-02-13T21:44:54.767656Z"
    }
   },
   "source": [
    "# check_particle_fit: a function that checks the fits\n",
    "\n",
    "### Josh's Fit checking function, updated on 9.4.2024. Added the function to select a certain particle to look at\n",
    "def check_particle_fit(binned_particles_fitted, show_denoised_plot=False):\n",
    "    '''\n",
    "    Check the fit of each particle. All particles are approved by default.\n",
    "    '''\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    particle_index = 0\n",
    "    particle_num = binned_particles_fitted.index.max()\n",
    "\n",
    "    # move to the first unchecked particle--------------------------------------\n",
    "    # first_flag = False\n",
    "    # while not first_flag:\n",
    "    #     particle_data = binned_particles_fitted[particle_index:particle_index+1]\n",
    "    #     status = particle_data['approval_status'].values[0]\n",
    "    #     if status == 0:\n",
    "    #         first_flag = True\n",
    "    #     else:\n",
    "    #         if particle_index < particle_num:\n",
    "    #             particle_index += 1\n",
    "    #         elif particle_index == particle_num:\n",
    "    #             warn('No particle has been left unchecked')\n",
    "    #             break\n",
    "    #---------------------------------------------------------------------------\n",
    "    \n",
    "    def update_plot(particle_index):\n",
    "        ax.clear()\n",
    "        try:\n",
    "            particle_data = binned_particles_fitted[particle_index:particle_index+1] # select the particle\n",
    "            \n",
    "            x = particle_data['t_s'].values[0]\n",
    "            y = particle_data['intensity_from_neighborhood'].values[0]\n",
    "            y_denoised = particle_data['tv_denoised_trace'].values[0]\n",
    "            y_err = particle_data['intensity_std_error_from_neighborhood'].values[0]\n",
    "    \n",
    "            # plot the particle trace with error bar along with the denoised trace\n",
    "            ax.errorbar(x, y, yerr=y_err, fmt=\".\", elinewidth=1, label='Data')\n",
    "            if show_denoised_plot:\n",
    "                ax.plot(x, y_denoised, color='k', label='TV denoised')\n",
    "    \n",
    "            # plot the half cycle fit\n",
    "            try:\n",
    "                fit_result = particle_data['fit_results'].values[0]\n",
    "                timepoints, t_interp, MS2, half_cycle_fit, [basal, t_on, t_dwell, rate, CI] = fit_result\n",
    "    \n",
    "                ax.plot(t_interp, half_cycle_fit, label=f'Fit (slope = {round(rate,2)})', linewidth=3)\n",
    "    \n",
    "                # ax.plot(t_interp, make_half_cycle(basal, t_on, t_dwell, rate, t_interp), label=f\"Fit (slope = {round(rate, 2)})\", linewidth=3, color='orange')\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "            particle = particle_data['particle'].values[0]\n",
    "            # bin = particle_data['bin'].values[0]\n",
    "            mean_x = (particle_data.loc[particle_data[\"particle\"] == particle, \"x\"]\n",
    "            .values[0]\n",
    "            .mean()\n",
    "            )\n",
    "            initial_frame = (compiled_dataframe.loc[compiled_dataframe[\"particle\"] == particle, \"frame\"].\n",
    "            values[0][0]\n",
    "            )\n",
    "            status = particle_data['approval_status'].values[0]\n",
    "    \n",
    "            if status == 1:\n",
    "                ax.set_facecolor((0.7, 1, 0.7)) # approve\n",
    "            elif status == -1:\n",
    "                ax.set_facecolor((1, 0.7, 0.7)) # reject color\n",
    "            elif status == 0:\n",
    "                ax.set_facecolor((1, 1, 1))\n",
    "            elif status == 2:\n",
    "                ax.set_facecolor((1, 1, 0.7))\n",
    "            \n",
    "            ax.set_title(f'Particle #{particle} ({particle_index+1}/{particle_num+1}), x = {np.round(mean_x, 2)}, Initial frame {initial_frame}')\n",
    "            ax.set_xlabel(\"Time (s)\")\n",
    "            ax.set_ylabel(\"Spot intensity (AU)\")\n",
    "            ax.legend()\n",
    "\n",
    "        except Exception as e:\n",
    "            particle = particle_data['particle'].values[0]\n",
    "            print(f\"Error processing particle {particle}: {e}\")\n",
    "\n",
    "        fig.canvas.draw()\n",
    "\n",
    "    def on_key(event):\n",
    "        nonlocal particle_index\n",
    "        if event.key == 'left':\n",
    "            particle_index = max(0, particle_index - 1)\n",
    "        elif event.key == 'right':\n",
    "            particle_index = min(len(binned_particles_fitted) - 1, particle_index + 1)\n",
    "        elif event.key == 'a':\n",
    "            binned_particles_fitted.at[particle_index, 'approval_status'] = 1\n",
    "        elif event.key == 'r':\n",
    "            binned_particles_fitted.at[particle_index, 'approval_status'] = -1\n",
    "        elif event.key == 'c':\n",
    "            binned_particles_fitted.at[particle_index, 'approval_status'] = 0\n",
    "        elif event.key == 'p':\n",
    "            binned_particles_fitted.at[particle_index, 'approval_status'] = 2\n",
    "        elif event.key == 'j':\n",
    "            # Create a Tkinter root window and hide it\n",
    "            root = tk.Tk()\n",
    "            root.withdraw()\n",
    "\n",
    "            # Ask for input\n",
    "            input_index = simpledialog.askinteger(\"Input\", \"Enter particle index:\", minvalue=0)#, maxvalue=particle_num)\n",
    "            \n",
    "            # Update the particle index if input is valid\n",
    "            if input_index is not None:\n",
    "                particle_index = binned_particles_fitted[binned_particles_fitted['particle'] == input_index].index.values[0]\n",
    "\n",
    "            # Destroy the Tkinter root window\n",
    "            root.destroy()\n",
    "        update_plot(particle_index)\n",
    "    update_plot(particle_index)\n",
    "    fig.canvas.mpl_connect('key_press_event', on_key)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect whether the trace checking has been done \"previously\". If so, load the previous results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T21:44:26.664780Z",
     "start_time": "2025-02-13T21:44:26.625535Z"
    }
   },
   "source": [
    "checked_traces_file_path = test_dataset_name + '/traces_compiled_dataframe_fits_checked.pkl'\n",
    "\n",
    "checked_traces_previous = os.path.isfile(checked_traces_file_path)\n",
    "checked_traces_previous"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T21:44:57.688723Z",
     "start_time": "2025-02-13T21:44:57.676451Z"
    }
   },
   "source": [
    "if checked_traces_previous:\n",
    "    # Load the DataFrame from the .pkl file\n",
    "    print('Load from previous trace checking results, which are shown below.')\n",
    "    traces_compiled_dataframe_fits_checked = pd.read_pickle(checked_traces_file_path)\n",
    "    traces_compiled_dataframe_fits_checked_temp = traces_compiled_dataframe_fits_checked.copy()\n",
    "    check_particle_fit(traces_compiled_dataframe_fits_checked_temp)\n",
    "\n",
    "else:\n",
    "    print('Do trace checking for the dataset')\n",
    "    check_particle_fit(traces_compiled_dataframe_fits)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do trace checking for the dataset\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'traces_compiled_dataframe_fits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDo trace checking for the dataset\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 10\u001B[0m     check_particle_fit(\u001B[43mtraces_compiled_dataframe_fits\u001B[49m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'traces_compiled_dataframe_fits' is not defined"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save the checked traces or update the checked traces if any changes are made\n",
    "\n",
    "if checked_traces_previous:\n",
    "    if all(traces_compiled_dataframe_fits_checked_temp.approval_status == traces_compiled_dataframe_fits_checked.approval_status):\n",
    "        print('No changes made to the trace checking results')\n",
    "    else:\n",
    "        answer = messagebox.askyesno('Question', 'Changes to the checked traces detected. Save the changes?')\n",
    "        if answer:\n",
    "            traces_compiled_dataframe_fits_checked_temp.to_pickle(checked_traces_file_path, compression=None)\n",
    "            print('Checked traces updated')\n",
    "        else:\n",
    "            print('No changes made to the trace checking results')\n",
    "\n",
    "else:\n",
    "    traces_compiled_dataframe_fits_checked = traces_compiled_dataframe_fits[traces_compiled_dataframe_fits['approval_status'] == 1].reset_index()\n",
    "    traces_compiled_dataframe_fits_checked.to_pickle(checked_traces_file_path, compression=None)\n",
    "    print('Checked traces saved')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort the traces by which bin they are in"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Sorting traces by the bin they belong to, and calculate the average of fit slope for each bin\n",
    "\n",
    "bin_width = 1/num_bins\n",
    "\n",
    "# Create an array to store the bin indices for each trace\n",
    "bin_indices = np.zeros(len(traces_compiled_dataframe_fits_checked))\n",
    "\n",
    "# Loop through traces_compiled_dataframe_fits_checked and assign each trace to a bin based on mean_ap position\n",
    "for i in range(len(traces_compiled_dataframe_fits_checked)):\n",
    "    particle = traces_compiled_dataframe_fits_checked['particle'][i]\n",
    "    bin_indices[i] = (\n",
    "        traces_compiled_dataframe_fits_checked.loc[traces_compiled_dataframe_fits_checked['particle'] == particle, 'ap']\n",
    "        .values[0].mean() // bin_width\n",
    "    )\n",
    "\n",
    "# Calculate the number of traces in each bin\n",
    "bin_counts = np.zeros(num_bins)\n",
    "for i in range(num_bins):\n",
    "    bin_counts[i] = np.sum(bin_indices == i)\n",
    "\n",
    "print(f'bin_counts = {bin_counts}')\n",
    "print(f'np.sum(bin_counts) = {np.sum(bin_counts)}')\n",
    "\n",
    "# Calculate the average fit rates for each bin and store particle IDs with rates in each bin\n",
    "mean_fit_rates = np.zeros(num_bins)\n",
    "bin_particles_rates = np.zeros(num_bins, dtype=object)\n",
    "SE_fit_rates = np.zeros(num_bins)\n",
    "for i in range(num_bins):\n",
    "    if bin_counts[i] == 0:\n",
    "        mean_fit_rates[i] = np.nan\n",
    "        continue\n",
    "    else:\n",
    "        rates = (\n",
    "            60*traces_compiled_dataframe_fits_checked.loc[bin_indices == i, 'fit_results'].apply(\n",
    "                lambda x: x[4][3] if x[4] is not None else np.nan).values\n",
    "            )\n",
    "        particles = (\n",
    "            traces_compiled_dataframe_fits_checked.loc[bin_indices == i, 'particle']\n",
    "            .values\n",
    "            )\n",
    "        \n",
    "        # Store the particle IDs with their rates in each bin for further analysis\n",
    "        bin_particles_rates[i] = {\n",
    "            'bin': i,\n",
    "            'particles': particles,\n",
    "            'rates': rates\n",
    "        }\n",
    "        \n",
    "        mean_fit_rates[i] = (np.nanmean(rates))\n",
    "\n",
    "        # Standard error of the mean\n",
    "        SE_fit_rates[i] = np.nanstd(rates) / np.sqrt(len(rates))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Prepare the data for plotting\n",
    "\n",
    "not_nan_1 = ~np.isnan(mean_fit_rates)\n",
    "\n",
    "bin_indices_1 = np.arange(num_bins)[not_nan_1]\n",
    "ap_positions_1 = bin_indices_1 * 1/num_bins\n",
    "\n",
    "bin_slopes_1 = mean_fit_rates[not_nan_1]\n",
    "bin_slope_errs_1 = SE_fit_rates[not_nan_1]\n",
    "\n",
    "max_bin_slope_1 = np.max(bin_slopes_1)\n",
    "ylim_up = 1.5*max_bin_slope_1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot the average slope of trace fits for each bin number\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(ap_positions_1, bin_slopes_1, yerr=bin_slope_errs_1, capsize=2, fmt='o')\n",
    "plt.xlabel('AP Position')\n",
    "plt.ylabel('Average rate of trace fits (AU/min)')\n",
    "plt.title('Average rate of trace fits vs. AP position')\n",
    "plt.ylim(0,ylim_up)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheme 2 (Average & Fit): Taking the Average of All MS2 Traces for Each Bin and then Fit to the Averaged Trace"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# bin_average_NC14: a function that bins all the traces and takes the average of the traces for each bin\n",
    "\n",
    "def bin_average_NC14(compiled_dataframe, NC14_start_frame, bin_num=num_bins, shift_to_same_start_frame=True):\n",
    "    '''\n",
    "    A function that bins all the traces and takes the average of the traces for each bin.\n",
    "\n",
    "    ARGUMENTS\n",
    "        bin_num: number of bins, default value is 42\n",
    "        shift_to_same_start_frame: if true, shift all NC14 particles so that they start at the same frame.\n",
    "\n",
    "    OUTPUT\n",
    "        a list, where each element is a pandas dataframe for one bin containing the average intensity of all \n",
    "        the traces in that bin. The average is taken frame-wise.\n",
    "    '''\n",
    "    \n",
    "    bin_width = 1/num_bins\n",
    "    \n",
    "    # Keep only NC14 particles\n",
    "    compiled_dataframe_NC14 = compiled_dataframe[compiled_dataframe['frame'].apply(min) >= NC14_start_frame]\n",
    "\n",
    "    # Create an array to store the bin indices for each trace\n",
    "    bin_indices = np.zeros(len(compiled_dataframe_NC14))\n",
    "    \n",
    "    # Loop through compiled_dataframe_NC14 and assign each trace to a bin based on mean ap position\n",
    "    for i in range(len(compiled_dataframe_NC14)):\n",
    "        particle = compiled_dataframe_NC14['particle'][i]\n",
    "        bin_indices[i] = (\n",
    "            compiled_dataframe_NC14.loc[compiled_dataframe_NC14['particle'] == particle, 'ap']\n",
    "            .values[0].mean() // bin_width\n",
    "        )\n",
    "        \n",
    "    compiled_dataframe_NC14['bin'] = bin_indices.astype(int)\n",
    "\n",
    "    # Shift all NC14 particles to the same start frame\n",
    "    if shift_to_same_start_frame:\n",
    "        for particle in range(len(compiled_dataframe_NC14)):\n",
    "            try:\n",
    "                frame_array = compiled_dataframe_NC14['frame'][particle][:]\n",
    "                first_frame = np.min(frame_array)\n",
    "                new_frame_array = frame_array - first_frame\n",
    "                compiled_dataframe_NC14['frame'][particle] = new_frame_array\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # sort particles by bins\n",
    "    binned_particles_NC14 = [None]*bin_num\n",
    "    for bin in range(bin_num): # for each bin\n",
    "        mask = compiled_dataframe_NC14[\"bin\"] == bin\n",
    "        binned_particles_NC14[bin] = compiled_dataframe_NC14[mask]\n",
    "\n",
    "    \n",
    "    # A function that sorts intensity data based on bin and frame, and for each bin, calculates the average intensity in each frame\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    def bin_average_process(binned_particles):\n",
    "    \n",
    "        # sort intensity data based on bin and frame\n",
    "        \n",
    "        intensity_by_frame = [[] for _ in range(bin_num)]\n",
    "        \n",
    "        for bin in range(bin_num):\n",
    "                \n",
    "            # find the max and min frame number in a bin\n",
    "            try:\n",
    "                max_frame = max(binned_particles[bin][\"frame\"].apply(np.max))\n",
    "                min_frame = min(binned_particles[bin][\"frame\"].apply(np.min))\n",
    "                \n",
    "                intensity_by_frame[bin] = pd.DataFrame({'frame': range(min_frame,max_frame), \n",
    "                                     'intensity': [[] for _ in range(0,max_frame-min_frame)], \n",
    "                                     'average_intensity': [None for _ in range(0,max_frame-min_frame)],\n",
    "                                    'std_err_intensity': [None for _ in range(0,max_frame-min_frame)]})\n",
    "                \n",
    "                bin_particle_num = binned_particles[bin].shape[0]\n",
    "                \n",
    "                for frame in range(min_frame, max_frame): # for each frame along the movie\n",
    "                    FrameIndex = frame - min_frame\n",
    "                    \n",
    "                    for particle in range(bin_particle_num): # for each particle in the bin\n",
    "                        particle_frames = np.array(binned_particles[bin])[particle][1] # extract the frame list of a single particle\n",
    "                        particle_intensity = np.array(binned_particles[bin])[particle][3] # extract the intensity list of a single particle\n",
    "    \n",
    "                        # avoid 0 dimension array that will create undesired results\n",
    "                        if particle_frames.ndim == 0:\n",
    "                            particle_frames = np.array([particle_frames])\n",
    "    \n",
    "                        if particle_intensity.ndim == 0:\n",
    "                            particle_intensity = np.array([particle_intensity])\n",
    "                        \n",
    "                        for el in range(len(particle_frames)): # for each frame of this particle\n",
    "                            if particle_frames[el] == frame:\n",
    "                                # add the intensity value of this particle at this frame to the new data structure\n",
    "                                intensity_by_frame[bin]['intensity'][FrameIndex].append(particle_intensity[el])\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "        \n",
    "        # for each bin, calculate the average intensity in each frame along with the standard error\n",
    "        \n",
    "        for bin in range(bin_num):\n",
    "            try:\n",
    "                frame_num = intensity_by_frame[bin].shape[0]\n",
    "                for frame in range(frame_num):\n",
    "                    intensity_data = intensity_by_frame[bin]['intensity'][frame]\n",
    "                    intensity_by_frame[bin]['average_intensity'][frame] = np.mean(intensity_data)\n",
    "                    intensity_by_frame[bin]['std_err_intensity'][frame] = np.std(intensity_data)/np.sqrt(len(intensity_data))\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "        return intensity_by_frame\n",
    "\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    return bin_average_process(binned_particles_NC14)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "bin_average_intensity_list = bin_average_NC14(compiled_dataframe, NC14_start_frame=nc14_start_frame)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(bin_average_intensity_list)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define fit functions\n",
    "Before running the cell below, please run the first cell in Scheme 1. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# fit_average_trace: a function that fits the average trace for a single bin\n",
    "\n",
    "def fit_average_trace(timepoints, MS2, tv_denoised_traces, MS2_std, bin_index):\n",
    "    # timepoints: the 'frame' column from intensity_by_frame multiplied by time_res_min\n",
    "    # MS2: the 'average_intensity' column\n",
    "    # tv_denoised_traces: the 'denoised_average_intensity' column\n",
    "    # MS2_std: the 'std_erfit_resultsr_intensity' column\n",
    "    \"\"\"\n",
    "    The single-trace version of fit_all_traces.\n",
    "    \"\"\"\n",
    "    fit_results = []\n",
    "\n",
    "    # Compute the first derivative of TV denoised with respect to time\n",
    "    dy_dx = first_derivative(timepoints, tv_denoised_traces)\n",
    "\n",
    "    # Compute the mean of the first derivative over intervals with constant sign\n",
    "    mean_dy_dx, change_indices = mean_sign_intervals(dy_dx)\n",
    "\n",
    "    # Keep datapoints from before first sign change\n",
    "    try:\n",
    "        timepoints = timepoints[:change_indices[0]]\n",
    "        MS2 = MS2[:change_indices[0]]\n",
    "        MS2_std = MS2_std[:change_indices[0]]\n",
    "\n",
    "        # Interpolate the timepoints\n",
    "        t_interp = np.linspace(min(timepoints), max(timepoints), 1000)\n",
    "    except:\n",
    "        print(f\"Failed to find derivative sign change for average trace {bin_index+1}\")\n",
    "\n",
    "\n",
    "    # Compute the fit values\n",
    "    try:\n",
    "        basal, t_on, t_dwell, rate, CI = fit_half_cycle(MS2, timepoints, t_interp, MS2_std)\n",
    "\n",
    "        fit_results = [timepoints,t_interp, MS2, make_half_cycle(basal, t_on, t_dwell, rate, t_interp),\n",
    "                            [basal, t_on, t_dwell, rate, CI]]\n",
    "    except Exception as e: \n",
    "        print(f\"Failed to fit average trace {bin_index+1}: {e}\")\n",
    "        fit_results = [timepoints,t_interp, MS2, None, None]\n",
    "\n",
    "    return fit_results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# fit_bin_averages: a function that fits to the average intensity plot for each bin and outputs the slopes of the fits\n",
    "\n",
    "def fit_bin_averages(bin_average_intensity_list, time_res_min=time_res_min):\n",
    "\n",
    "    bin_num = len(bin_average_intensity_list)\n",
    "\n",
    "    fit_results = [None]*bin_num\n",
    "    fit_slopes = [np.nan]*bin_num\n",
    "\n",
    "    for bin in range(bin_num):\n",
    "        try:\n",
    "            bin_average_intensity = bin_average_intensity_list[bin]\n",
    "    \n",
    "            bin_average_intensity_denoised = denoise_bin_average_intensity(bin_average_intensity)\n",
    "\n",
    "            # Prepare the lists/arrays needed for the fit\n",
    "            first_frame = bin_average_intensity_denoised['frame'][0]\n",
    "            x = (bin_average_intensity_denoised['frame'].values - first_frame) * time_res_min\n",
    "            y = bin_average_intensity_denoised['average_intensity'].values\n",
    "            y_err = bin_average_intensity_denoised['std_err_intensity'].values\n",
    "            y_denoised = bin_average_intensity_denoised['denoised_average_intensity'].values\n",
    "\n",
    "            # Generate the fit\n",
    "            fit_result = fit_average_trace(x, y, y_denoised, y_err, bin)\n",
    "\n",
    "            # Store the fit result and the fit slope\n",
    "            fit_results[bin] = fit_result\n",
    "            \n",
    "            try:\n",
    "                # Store the fit slope\n",
    "                timepoints, t_interp, MS2, half_cycle_fit, [basal, t_on, t_dwell, rate, CI] = fit_result\n",
    "                fit_slopes[bin] = rate\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return fit_results, np.array(fit_slopes)\n",
    "\n",
    "\n",
    "# A function that denoise the dataframes in bin_average_intensity_list, used in the function fit_bin_average\n",
    "\n",
    "def denoise_bin_average_intensity(bin_average_intensity):\n",
    "\n",
    "    bin_average_intensity_denoised = bin_average_intensity.copy()\n",
    "    \n",
    "    try:\n",
    "        bin_average_intensity_denoised = bin_average_intensity_denoised.dropna(subset=['average_intensity']) # remove all nan\n",
    "\n",
    "        # denoise\n",
    "        before_denoise = np.array(list(bin_average_intensity_denoised['average_intensity']))\n",
    "        after_denoise = denoise_tv_chambolle(before_denoise, weight=1080, max_num_iter=500)\n",
    "        \n",
    "        bin_average_intensity_denoised['denoised_average_intensity'] = after_denoise\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "    return bin_average_intensity_denoised"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fit_results, fit_slopes = fit_bin_averages(bin_average_intensity_list)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bin_average_intensity = bin_average_intensity_list[6]\n",
    "\n",
    "bin_average_intensity_denoised = denoise_bin_average_intensity(bin_average_intensity)\n",
    "\n",
    "# Prepare the lists/arrays needed for the fit\n",
    "first_frame = bin_average_intensity_denoised['frame'][0]\n",
    "x = (bin_average_intensity_denoised['frame'].values - first_frame) * time_res_min\n",
    "y = bin_average_intensity_denoised['average_intensity'].values\n",
    "y_err = bin_average_intensity_denoised['std_err_intensity'].values\n",
    "y_denoised = bin_average_intensity_denoised['denoised_average_intensity'].values\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cut = 2\n",
    "head = 0\n",
    "\n",
    "mask = (x >= head) & (x <= cut)\n",
    "\n",
    "new_x = x[mask]\n",
    "new_y = y[mask]\n",
    "new_yerr = y_err[mask]\n",
    "plt.figure()\n",
    "plt.scatter(x, y)\n",
    "plt.scatter(new_x, new_y)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "x_interp = np.linspace(min(new_x), max(new_x), 1000)\n",
    "\n",
    "fit_half_cycle(new_y, new_x, x_interp, new_yerr)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate the fit\n",
    "fit_result = fit_average_trace(x, y, y_denoised, y_err, bin)\n",
    "\n",
    "# Store the fit result and the fit slope\n",
    "fit_results[bin] = fit_result"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the fit to each bin"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# check_bin_fit: a function that checks the fit for each bin\n",
    "\n",
    "def check_bin_fit(bin_average_intensity_list, fit_results, time_res_min=time_res_min, show_denoised_plot=False, show_fit=True):\n",
    "    '''\n",
    "    Check the fit of each particle. All particles are approved by default.\n",
    "\n",
    "    Note that the bin index shown in the title and the total bin number start from 0, \n",
    "    i.e. if it's 2/10 it means it's actually the third bin out of 11 bins.\n",
    "    '''\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    bin_index = 0\n",
    "    bin_num =len(bin_average_intensity_list) \n",
    "\n",
    "    # move to the first unchecked particle--------------------------------------\n",
    "    # first_flag = False\n",
    "    # while not first_flag:\n",
    "    #     particle_data = binned_particles_fitted[particle_index:particle_index+1]\n",
    "    #     status = particle_data['approval_status'].values[0]\n",
    "    #     if status == 0:\n",
    "    #         first_flag = True\n",
    "    #     else:\n",
    "    #         if particle_index < particle_num:\n",
    "    #             particle_index += 1\n",
    "    #         elif particle_index == particle_num:\n",
    "    #             warn('No particle has been left unchecked')\n",
    "    #             break\n",
    "    #---------------------------------------------------------------------------\n",
    "    # import matplotlib\n",
    "    # plt.close('all')\n",
    "    \n",
    "    def update_plot(bin_index):\n",
    "        ax.clear()\n",
    "        try:\n",
    "            bin_average_intensity = bin_average_intensity_list[bin_index] # select the particle\n",
    "\n",
    "            bin_average_intensity_denoised = denoise_bin_average_intensity(bin_average_intensity)\n",
    "            \n",
    "            x = bin_average_intensity_denoised['frame'].values * time_res_min\n",
    "            y = bin_average_intensity_denoised['average_intensity'].values\n",
    "            y_err = bin_average_intensity_denoised['std_err_intensity'].values\n",
    "            y_denoised = bin_average_intensity_denoised['denoised_average_intensity'].values\n",
    "    \n",
    "            # plot the particle trace with error bar along with the denoised trace\n",
    "            ax.errorbar(x, y, yerr=y_err, fmt=\".\", elinewidth=1, label='Data')\n",
    "\n",
    "            if show_denoised_plot:\n",
    "                ax.plot(x, y_denoised, color='k', label='TV denoised')\n",
    "    \n",
    "            # plot the half cycle fit\n",
    "            try:\n",
    "                fit_result = fit_results[bin_index]\n",
    "                timepoints, t_interp, MS2, half_cycle_fit, [basal, t_on, t_dwell, rate, CI] = fit_result\n",
    "    \n",
    "                if show_fit:\n",
    "                    ax.plot(t_interp, half_cycle_fit, label=f'Fit (slope = {round(rate,2)})', linewidth=3)\n",
    "    \n",
    "                # ax.plot(t_interp, make_half_cycle(basal, t_on, t_dwell, rate, t_interp), label=f\"Fit (slope = {round(rate, 2)})\", linewidth=3, color='orange')\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "            # status = particle_data['approval_status'].values[0]\n",
    "    \n",
    "            # if status == 1:\n",
    "            #     ax.set_facecolor((0.7, 1, 0.7)) # approve\n",
    "            # elif status == -1:\n",
    "            #     ax.set_facecolor((1, 0.7, 0.7)) # reject color\n",
    "            # elif status == 0:\n",
    "            #     ax.set_facecolor((1, 1, 1))\n",
    "            # elif status == 2:\n",
    "            #     ax.set_facecolor((1, 1, 0.7))\n",
    "                \n",
    "            \n",
    "            ax.set_title(f'Bin #{bin_index+1}/{bin_num-1}')\n",
    "            ax.set_xlabel(\"Time (min)\")\n",
    "            ax.set_xlim(0, 14)\n",
    "            ax.set_ylabel(\"Spot intensity (AU)\")\n",
    "            ax.legend()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing bin {bin_index}: {e}\")\n",
    "\n",
    "        fig.canvas.draw()\n",
    "\n",
    "    def on_key(event):\n",
    "        nonlocal bin_index\n",
    "        if event.key == 'left':\n",
    "            bin_index = max(0, bin_index - 1)\n",
    "        elif event.key == 'right':\n",
    "            bin_index = min(len(bin_average_intensity_list) - 1, bin_index + 1)\n",
    "        # elif event.key == 'a':\n",
    "        #     bin_average_intensity_list.at[bin_index, 'approval_status'] = 1\n",
    "        # elif event.key == 'r':\n",
    "        #     bin_average_intensity_list.at[bin_index, 'approval_status'] = -1\n",
    "        # elif event.key == 'c':\n",
    "        #     bin_average_intensity_list.at[bin_index, 'approval_status'] = 0\n",
    "        # elif event.key == 'p':\n",
    "        #     bin_average_intensity_list.at[bin_index, 'approval_status'] = 2\n",
    "        update_plot(bin_index)\n",
    "\n",
    "    update_plot(bin_index)\n",
    "    fig.canvas.mpl_connect('key_press_event', on_key)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "check_bin_fit(bin_average_intensity_list, fit_results, show_denoised_plot=False, show_fit=True)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot slope vs. AP position"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Prepare the data to be plotted\n",
    "\n",
    "not_nan_2 = ~np.isnan(fit_slopes)\n",
    "\n",
    "bin_indices_2 = np.arange(num_bins)[not_nan_2]\n",
    "bin_slopes_2 = fit_slopes[not_nan_2]\n",
    "\n",
    "# extract the widths of confidence intervals for the slopes, stored in bin_slope_errs\n",
    "bin_slope_errs_2 = np.zeros((num_bins, 2))\n",
    "for bin in range(num_bins):\n",
    "    try:\n",
    "        bin_slope_err = fit_results[bin][-1][-1][-1]\n",
    "        bin_slope_errs_2[bin] = bin_slope_err\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "bin_slope_errs_2 = np.transpose(np.abs(bin_slope_errs_2[not_nan_2] - bin_slopes_2[:, np.newaxis]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot the bins with data\n",
    "\n",
    "# Adjust the range of the bins if necessary. Default is start to end\n",
    "start = 0\n",
    "end = len(bin_indices_2)\n",
    "\n",
    "ap_positions_2 = bin_indices_2 * 1/num_bins\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(ap_positions_2[start:end], bin_slopes_2[start:end], yerr=bin_slope_errs_2[:,start:end], capsize=2, fmt='o')\n",
    "plt.xlabel('AP position')\n",
    "plt.ylabel('Fit rate of average trace (AU/min)')\n",
    "plt.title('Fit rate of average trace vs. AP position (with shifting)')\n",
    "plt.ylim(0, ylim_up)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlaying the results from the two schemes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure()\n",
    "plt.xlabel('AP Position')\n",
    "plt.ylabel('Rate (AU/min)')\n",
    "plt.title('Rate vs. AP position')\n",
    "plt.ylim(0, 120)\n",
    "\n",
    "# Scheme 1: Fit & Average\n",
    "plt.errorbar(ap_positions_1, bin_slopes_1, yerr=bin_slope_errs_1, capsize=2, fmt='o-', label='fit & average')\n",
    "\n",
    "# Scheme 2: Average & Fit\n",
    "plt.errorbar(ap_positions_2[start:end], bin_slopes_2[start:end], yerr=bin_slope_errs_2[:,start:end], capsize=2, fmt='o-', label='average & fit')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
