{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T21:46:57.584213Z",
     "start_time": "2024-12-17T21:46:52.713879Z"
    }
   },
   "source": [
    "# Import pipeline\n",
    "from transcription_pipeline import nuclear_pipeline\n",
    "from transcription_pipeline import preprocessing_pipeline\n",
    "\n",
    "from transcription_pipeline import spot_pipeline\n",
    "from transcription_pipeline import fullEmbryo_pipeline\n",
    "\n",
    "# Importing libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transcription_pipeline.spot_analysis import compile_data\n",
    "from transcription_pipeline.utils import plottable\n",
    "\n",
    "from scipy.signal import medfilt\n",
    "from skimage.restoration import denoise_tv_chambolle\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import least_squares\n",
    "from scipy.stats import chi2\n",
    "import pandas as pd\n",
    "import emcee"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`JAVA_HOME` environment variable set to /mnt/Data1/Nick/miniforge3/envs/transcription_pipeline\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T21:47:00.291658Z",
     "start_time": "2024-12-17T21:47:00.288434Z"
    }
   },
   "source": [
    "RBSPWM_datasets = [\n",
    "    \"test_data/2024-02-26/Halo-RBSPWM_embryo01\",\n",
    "    \"test_data/2024-02-26/Halo-RBSPWM_embryo02\",\n",
    "    \"test_data/2024-05-07/Halo552-RBSPWM_embryo01\",\n",
    "    \"test_data/2024-05-07/Halo552-RBSPWM_embryo02\",\n",
    "    \"test_data/2024-05-09/Halo552-RBSPWM_embryo01\",\n",
    "    \"../../Josh/transcription_pipeline-main/test_data/2024-05-09/Halo552-RBSPWM_embryo01/\"\n",
    "]\n",
    "\n",
    "RBSVar2_datasets = [\n",
    "    \"test_data/2024-07-23/Halo673_RBSVar2_embryo01\",\n",
    "    \"test_data/2024-07-25/Halo673_RBSVar2_embryo01\"\n",
    "]\n",
    "\n",
    "MCP_mSG_datasets = [\n",
    "    \"test_data/2024-08-13/MCP-mSG,ParB-mScar_normWindow\",\n",
    "    \"test_data/2024-10-31/MCP-mSG_ParB-mScar_RBSPWM_embryo01\",\n",
    "    \"test_data/2024-10-31/MCP-mSG_ParB-mScar_RBSPWM_embryo02\",\n",
    "    \"test_data/2024-11-05/MCP-mSG_ParB-mScar_RBSPWM_embryo01\",\n",
    "    \"test_data/2024-11-18/MCP-mSG_ParB-mScar_RBSPWM_embryo01\",\n",
    "    \"test_data/2024-11-18/MCP-mSG_ParB-mScar_RBSPWM_embryo02\",\n",
    "    ]\n",
    "\n",
    "test_dataset_name = MCP_mSG_datasets[4]\n",
    "print(test_dataset_name)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data/2024-11-18/MCP-mSG_ParB-mScar_RBSPWM_embryo01\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import MS2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T22:00:14.167143Z",
     "start_time": "2024-12-17T21:47:09.588009Z"
    }
   },
   "source": [
    "## Import from scratch\n",
    "dataset = preprocessing_pipeline.DataImport(\n",
    "    name_folder=test_dataset_name,\n",
    "    trim_series=True,\n",
    "    working_storage_mode='zarr',\n",
    "    # import_previous=True, \n",
    ")\n",
    "dataset.save();"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/preprocessing/import_data.py:863: UserWarning: Could not align z-stack after series 0.\n",
      "  warnings.warn(\n",
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/preprocessing/import_data.py:863: UserWarning: Could not align z-stack after series 1.\n",
      "  warnings.warn(\n",
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/preprocessing/import_data.py:863: UserWarning: Could not align z-stack after series 2.\n",
      "  warnings.warn(\n",
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/preprocessing/import_data.py:863: UserWarning: Could not align z-stack after series 3.\n",
      "  warnings.warn(\n",
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/preprocessing/import_data.py:863: UserWarning: Could not align z-stack after series 4.\n",
      "  warnings.warn(\n",
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/preprocessing/import_data.py:863: UserWarning: Could not align z-stack after series 5.\n",
      "  warnings.warn(\n",
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/preprocessing/import_data.py:863: UserWarning: Could not align z-stack after series 6.\n",
      "  warnings.warn(\n",
      "/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/preprocessing/import_data.py:863: UserWarning: Could not align z-stack after series 7.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import FullEmbryo Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T21:47:07.127137Z",
     "start_time": "2024-12-17T21:47:06.863551Z"
    }
   },
   "source": [
    "FullEmbryo_dataset = preprocessing_pipeline.FullEmbryoImport(\n",
    "    name_folder=test_dataset_name,\n",
    "    #import_previous=True\n",
    ")\n",
    "FullEmbryo_dataset.save()"
   ],
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m FullEmbryo_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mpreprocessing_pipeline\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFullEmbryoImport\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mname_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_dataset_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#import_previous=True\u001B[39;49;00m\n\u001B[1;32m      4\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m FullEmbryo_dataset\u001B[38;5;241m.\u001B[39msave()\n",
      "File \u001B[0;32m/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/preprocessing_pipeline.py:362\u001B[0m, in \u001B[0;36mFullEmbryoImport.__init__\u001B[0;34m(self, name_folder, import_previous)\u001B[0m\n\u001B[1;32m    353\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    354\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname_folder \u001B[38;5;241m=\u001B[39m name_folder\n\u001B[1;32m    356\u001B[0m     (\n\u001B[1;32m    357\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchannels_full_dataset_mid,\n\u001B[1;32m    358\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moriginal_global_metadata_mid,\n\u001B[1;32m    359\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moriginal_frame_metadata_mid,\n\u001B[1;32m    360\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexport_global_metadata_mid,\n\u001B[1;32m    361\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexport_frame_metadata_mid,\n\u001B[0;32m--> 362\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[43mimport_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_full_embryo\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname_folder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mMid*\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    364\u001B[0m     (\n\u001B[1;32m    365\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchannels_full_dataset_surf,\n\u001B[1;32m    366\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moriginal_global_metadata_surf,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    369\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexport_frame_metadata_surf,\n\u001B[1;32m    370\u001B[0m     ) \u001B[38;5;241m=\u001B[39m import_data\u001B[38;5;241m.\u001B[39mimport_full_embryo(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname_folder, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSurf*\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/mnt/Data1/Nick/transcription_pipeline/transcription_pipeline/preprocessing/import_data.py:1038\u001B[0m, in \u001B[0;36mimport_full_embryo\u001B[0;34m(name_folder, name)\u001B[0m\n\u001B[1;32m   1036\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMultiple Mid images found in folder.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     file \u001B[38;5;241m=\u001B[39m \u001B[43mfile_list\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m   1040\u001B[0m \u001B[38;5;66;03m# Pull the data file as a pipeline object\u001B[39;00m\n\u001B[1;32m   1041\u001B[0m file_image \u001B[38;5;241m=\u001B[39m pims\u001B[38;5;241m.\u001B[39mBioformats(file, read_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjpype\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from Zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load MS2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "## Load from zarr\n",
    "dataset = preprocessing_pipeline.DataImport(\n",
    "    name_folder=test_dataset_name,\n",
    "    trim_series=True,\n",
    "    # working_storage_mode='zarr',\n",
    "    import_previous=True, \n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load FullEmbryo Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# FullEmbryo_dataset = preprocessing_pipeline.FullEmbryoImport(\n",
    "#     name_folder=test_dataset_name,\n",
    "#     import_previous=True\n",
    "# )\n",
    "# Not working currently, but reported to Yovan where it only reads in the last channel\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting a DASK Client for parallel processing"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from dask.distributed import LocalCluster\n",
    "\n",
    "# Get the list of clusters\n",
    "active_clusters = LocalCluster._instances\n",
    "\n",
    "# Print details of active clusters\n",
    "for i, cluster in enumerate(active_clusters, 1):\n",
    "    print(f\"Cluster {i}:\")\n",
    "    print(f\"  Address: {cluster.scheduler_address}\")\n",
    "    print(f\"  Workers: {len(cluster.workers)}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from dask.distributed import LocalCluster, Client\n",
    "\n",
    "try:\n",
    "    cluster = LocalCluster(\n",
    "        host=\"localhost\",\n",
    "        scheduler_port=37764,\n",
    "        threads_per_worker=1,\n",
    "        n_workers=14,\n",
    "        memory_limit=\"6GB\",\n",
    "    )\n",
    "    \n",
    "    client = Client(cluster)\n",
    "except:\n",
    "    print(\"Cluster already running\")\n",
    "    client = Client('localhost:37763')\n",
    "\n",
    "print(client)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "client.restart()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(client.dashboard_link)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "client.shutdown()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuclear Tracking"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "nuclear_tracking = nuclear_pipeline.Nuclear(\n",
    "    data=dataset.channels_full_dataset[0],\n",
    "    global_metadata=dataset.export_global_metadata[0],\n",
    "    frame_metadata=dataset.export_frame_metadata[0],\n",
    "    series_splits=dataset.series_splits,\n",
    "    series_shifts=dataset.series_shifts,\n",
    "    search_range_um=1.5,\n",
    "    stitch=False,\n",
    "    stitch_max_distance=4,\n",
    "    stitch_max_frame_distance=2,\n",
    "    client=client,\n",
    "    keep_futures=False,\n",
    ")\n",
    "\n",
    "nuclear_tracking.track_nuclei(\n",
    "        working_memory_mode=\"zarr\",\n",
    "        working_memory_folder=test_dataset_name,\n",
    "        trackpy_log_path=\"\".join([test_dataset_name, \"trackpy_log\"]),\n",
    "    )\n",
    "    # Saves tracked nuclear mask as a zarr, and pickles dataframes with segmentation and\n",
    "    # tracking information.\n",
    "nuclear_tracking.save_results(\n",
    "        name_folder=test_dataset_name, save_array_as=None\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load nuclear tracking results\n",
    "nuclear_tracking = nuclear_pipeline.Nuclear()\n",
    "nuclear_tracking.read_results(name_folder=test_dataset_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spot Tracking"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "spot_tracking = spot_pipeline.Spot(\n",
    "    data=dataset.channels_full_dataset[0],\n",
    "    global_metadata=dataset.export_global_metadata[0],\n",
    "    frame_metadata=dataset.export_frame_metadata[0],\n",
    "    labels=None,#nuclear_tracking.reordered_labels,\n",
    "    expand_distance=3,\n",
    "    search_range_um=4.2,\n",
    "    retrack_search_range_um=4.5,\n",
    "    threshold_factor=1.3,\n",
    "    memory=3,\n",
    "    retrack_after_filter=False,\n",
    "    stitch=True,\n",
    "    min_track_length=0,\n",
    "    series_splits=dataset.series_splits,\n",
    "    series_shifts=dataset.series_shifts,\n",
    "    keep_bandpass=False,\n",
    "    keep_futures=False,\n",
    "    keep_spot_labels=False,\n",
    "    evaluate=True,\n",
    "    retrack_by_intensity=True,\n",
    "    client=client,\n",
    ")\n",
    "\n",
    "spot_tracking.extract_spot_traces(\n",
    "    working_memory_folder=test_dataset_name, \n",
    "    stitch=True,\n",
    "    retrack_after_filter=True,\n",
    "    trackpy_log_path = test_dataset_name+'/trackpy_log'\n",
    ")\n",
    "\n",
    "# Saves tracked spot mask as a zarr, and pickles dataframes with spot fitting and\n",
    "# quantification information.\n",
    "spot_tracking.save_results(name_folder=test_dataset_name, save_array_as=None)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Compiled Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load spot tracking results\n",
    "track_spots = spot_pipeline.Spot()\n",
    "track_spots.read_results(name_folder=test_dataset_name)\n",
    "\n",
    "# Load spot tracking dataframe\n",
    "spot_df = track_spots.spot_dataframe\n",
    "\n",
    "# Remove spots that were not detected\n",
    "detected_spots = spot_df[spot_df[\"particle\"] != 0]\n",
    "\n",
    "# Compile traces\n",
    "compiled_dataframe = compile_data.compile_traces(\n",
    "    detected_spots,\n",
    "    compile_columns_spot=[\n",
    "        \"frame\",\n",
    "        \"t_s\",\n",
    "        \"intensity_from_neighborhood\",\n",
    "        \"intensity_std_error_from_neighborhood\",\n",
    "        \"x\",\n",
    "        \"y\"\n",
    "    ],\n",
    "    nuclear_tracking_dataframe=None,\n",
    ")\n",
    "compiled_dataframe.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "signal = np.array(detected_spots['intensity_from_neighborhood'])\n",
    "background = np.array(detected_spots['background_intensity_from_neighborhood'])\n",
    "from matplotlib.colors import LogNorm\n",
    "print(signal.shape, background.shape)\n",
    "\n",
    "plt.hist2d(x=background, y=signal,\n",
    "           norm=LogNorm(vmin=1, vmax=1000), cmap='viridis',\n",
    "           alpha=1, bins=200)\n",
    "plt.ylim(-10,1200)\n",
    "plt.colorbar()\n",
    "plt.xlabel('Background (AU)')\n",
    "plt.ylabel('Signal (AU)')\n",
    "plt.title('Signal vs Background')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x=background, y=signal, alpha=0.01)\n",
    "plt.colorbar()\n",
    "plt.xlabel('Background')\n",
    "plt.ylabel('Signal')\n",
    "plt.title('Signal vs Background')\n",
    "plt.show()\n",
    "\n",
    "# Compute the correlation between signal and background\n",
    "#np.corrcoef(signal, background)\n",
    "print(np.corrcoef(signal, background))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Embryo Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(FullEmbryo_dataset.channels_full_dataset_surf[0][0, :, :], cmap='gray')\n",
    "plt.title('Full Embryo Surf')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(FullEmbryo_dataset.channels_full_dataset_mid[0][0, :, :], cmap='gray')\n",
    "plt.title('Full Embryo Mid')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fullEmbryo = fullEmbryo_pipeline.FullEmbryo(FullEmbryo_dataset, dataset, his_channel=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fullEmbryo.find_ap_axis(make_plots=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "compiled_dataframe = fullEmbryo.xy_to_ap(compiled_dataframe)\n",
    "compiled_dataframe.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Traces"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Restrict to longer traces\n",
    "min_frames = 80\n",
    "traces_compiled_dataframe = compiled_dataframe[\n",
    "    compiled_dataframe[\"frame\"].apply(lambda x: x.size) > min_frames\n",
    "]\n",
    "plt.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# %matplotlib widget\n",
    "# This is taken from https://stackoverflow.com/questions/18390461/scroll-backwards-and-forwards-through-matplotlib-plots\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from scipy.signal import medfilt\n",
    "from skimage.restoration import denoise_tv_chambolle\n",
    "\n",
    "traces = plottable.generate_trace_plot_list(traces_compiled_dataframe)\n",
    "#median_filtered_traces = [medfilt(trace[1], kernel_size=15) for trace in traces]\n",
    "\n",
    "tv_denoised_traces = [\n",
    "    denoise_tv_chambolle(trace[1], weight=1080, max_num_iter=500) for trace in traces\n",
    "]\n",
    "# potts_steps_traces = [\n",
    "#     potts_l1.l1_potts_step_detection(trace[1], gamma=-5e3, weights=(1 / trace[2] ** 2))\n",
    "#     for trace in traces\n",
    "# ]\n",
    "\n",
    "\n",
    "# plt.close()\n",
    "# plt.plot(traces[curr_pos][0], traces[curr_pos][1], label=\"Original\")otts_l1.l1_potts_step_detection(trace[1], gamma=-5e3, weights=(1 / trace[2] ** 2))\n",
    "#     for trace in traces\n",
    "# ]\n",
    "\n",
    "curr_pos = 0\n",
    "\n",
    "\n",
    "def key_event(e):\n",
    "    global curr_pos\n",
    "\n",
    "    if e.key == \"right\":\n",
    "        curr_pos = curr_pos + 1\n",
    "    elif e.key == \"left\":\n",
    "        curr_pos = curr_pos - 1\n",
    "    else:\n",
    "        return\n",
    "    curr_pos = curr_pos % len(traces)\n",
    "\n",
    "    ax.cla()\n",
    "    ax.errorbar(\n",
    "        traces[curr_pos][0],\n",
    "        traces[curr_pos][1],\n",
    "        yerr=traces[curr_pos][2],\n",
    "        fmt=\".\",\n",
    "        elinewidth=1,\n",
    "    )\n",
    "    # ax.plot(traces[curr_pos][0], median_filtered_traces[curr_pos], color=\"k\")\n",
    "    # ax.plot(traces[curr_pos][0], tv_denoised_traces[curr_pos], color=\"k\", label=\"TV\")\n",
    "    # ax.step(\n",
    "    #     traces[curr_pos][0],\n",
    "    #     potts_steps_traces[curr_pos],\n",
    "    #     where=\"mid\",\n",
    "    #     color=\"red\",\n",
    "    #     label=\"Potts L1\",\n",
    "    # )\n",
    "    ax.set_xlabel(\"time (s)\")\n",
    "    ax.set_ylabel(\"Spot intensity (AU)\")\n",
    "\n",
    "    particle = traces[curr_pos][3]\n",
    "    mean_x = (\n",
    "        compiled_dataframe.loc[compiled_dataframe[\"particle\"] == particle, \"x\"]\n",
    "        .values[0]\n",
    "        .mean()\n",
    "    )\n",
    "    initial_frame = (\n",
    "    compiled_dataframe.loc[compiled_dataframe[\"particle\"] == particle, \"frame\"]\n",
    "    .values[0][0]\n",
    "    )\n",
    "    ax.set_title(f\"Particle {particle}, x = {mean_x}, Initial frame {initial_frame}\")\n",
    "    ax.legend()\n",
    "    fig.canvas.draw()\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.canvas.mpl_connect(\"key_press_event\", key_event)\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "ax.errorbar(\n",
    "    traces[curr_pos][0],\n",
    "    traces[curr_pos][1],\n",
    "    yerr=traces[curr_pos][2],\n",
    "    fmt=\".\",\n",
    "    elinewidth=1,\n",
    ")\n",
    "# ax.plot(traces[curr_pos][0], median_filtered_traces[curr_pos], color=\"k\")\n",
    "ax.plot(traces[curr_pos][0], tv_denoised_traces[curr_pos], color=\"k\", label=\"TV\")\n",
    "# ax.step(\n",
    "#     traces[curr_pos][0],\n",
    "#     potts_steps_traces[curr_pos],\n",
    "#     where=\"mid\",\n",
    "#     color=\"red\",\n",
    "#     label=\"Potts L1\",\n",
    "# )\n",
    "ax.set_xlabel(\"time (s)\")\n",
    "ax.set_ylabel(\"Spot intensity (AU)\")\n",
    "\n",
    "particle = traces[curr_pos][3]\n",
    "mean_x = (\n",
    "    compiled_dataframe.loc[compiled_dataframe[\"particle\"] == particle, \"x\"]\n",
    "    .values[0]\n",
    "    .mean()\n",
    ")\n",
    "initial_frame = (\n",
    "    compiled_dataframe.loc[compiled_dataframe[\"particle\"] == particle, \"frame\"]\n",
    "    .values[0][0]\n",
    ")\n",
    "ax.set_title(f\"Particle {particle}, x = {mean_x}, Initial frame {initial_frame}\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Fit Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import least_squares\n",
    "from scipy.stats import chi2\n",
    "import pandas as pd\n",
    "import emcee\n",
    "\n",
    "# Version with normalization and regularization\n",
    "def make_half_cycle(basal, t_on, t_dwell, rate, t_interp):\n",
    "    half_cycle = np.zeros_like(t_interp)\n",
    "    half_cycle[t_interp < t_on] = basal\n",
    "    half_cycle[(t_interp >= t_on) & (t_interp < t_on + t_dwell)] = basal + rate * (t_interp[(t_interp >= t_on) & (t_interp < t_on + t_dwell)] - t_on)\n",
    "    half_cycle[t_interp >= t_on + t_dwell] = basal + rate * t_dwell\n",
    "    return half_cycle\n",
    "\n",
    "def fit_func(params, MS2, timepoints, t_interp):\n",
    "    return np.interp(timepoints, t_interp, make_half_cycle(*params, t_interp)) - MS2\n",
    "\n",
    "def initial_guess(MS2, timepoints):\n",
    "    # Initial guess for the parameters\n",
    "    basal0 = MS2[0]\n",
    "    t_on0 = timepoints[0]\n",
    "    t_dwell0 = (2/3)*(timepoints[-1]-timepoints[0])\n",
    "    rate0 = 1\n",
    "    # print(np.max(mean_dy_dx))\n",
    "    return [basal0, t_on0, t_dwell0, rate0]\n",
    "\n",
    "\n",
    "def fit_half_cycle(MS2, timepoints, t_interp, std_errors, max_nfev=3000):\n",
    "    # Initial guess\n",
    "    x0 = initial_guess(MS2, timepoints)\n",
    "    \n",
    "    # Parameter bounds\n",
    "    lb = [np.min(MS2), 0, 0, 0]  # Ensure t_dwell is non-negative\n",
    "    ub = [np.max(MS2), np.max(timepoints), np.max(timepoints), 1e7]\n",
    "\n",
    "    # Scaling factors to normalize parameters\n",
    "    scale_factors = np.array([np.max(MS2), np.max(timepoints), np.max(timepoints), 100])\n",
    "\n",
    "    # Scaled bounds\n",
    "    lb_scaled = np.array(lb) / scale_factors\n",
    "    ub_scaled = np.array(ub) / scale_factors\n",
    "    x0_scaled = np.array(x0) / scale_factors\n",
    "\n",
    "    # Scaled fit function\n",
    "    def fit_func_scaled(params, MS2, timepoints, t_interp):\n",
    "        params_unscaled = params * scale_factors\n",
    "        return fit_func(params_unscaled, MS2, timepoints, t_interp)\n",
    "\n",
    "    # Negative log-likelihood function\n",
    "    def negative_log_likelihood(params, MS2, timepoints, t_interp, std_errors, reg=1e-3):\n",
    "        residuals = fit_func_scaled(params, MS2, timepoints, t_interp) / std_errors\n",
    "        regularization = reg * np.sum(params[:]**2)\n",
    "        nll = 0.5 * np.sum(residuals**2) + regularization\n",
    "        return nll\n",
    "\n",
    "    # Initial parameter estimation using least_squares\n",
    "    res = least_squares(negative_log_likelihood, \n",
    "                        x0_scaled, bounds=(lb_scaled, ub_scaled), \n",
    "                        args=(MS2, timepoints, t_interp, std_errors), max_nfev=max_nfev)\n",
    "    \n",
    "\n",
    "    # Define log-probability function for MCMC\n",
    "    def log_prob(params, MS2, timepoints, t_interp, std_errors, scale_factors, lb_scaled, ub_scaled):\n",
    "        if np.any(params < lb_scaled) or np.any(params > ub_scaled):\n",
    "            return -np.inf\n",
    "        nll = negative_log_likelihood(params, MS2, timepoints, t_interp, std_errors)\n",
    "        return -nll  # Convert to log-probability\n",
    "\n",
    "    # MCMC parameters\n",
    "    nwalkers = 10\n",
    "    ndim = len(x0_scaled)\n",
    "    nsteps = 1000\n",
    "    initial_pos = res.x + 1e-4 * np.random.randn(nwalkers, ndim)\n",
    "    # Run MCMC\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, log_prob, args=(MS2, timepoints,\n",
    "                                                                    t_interp, std_errors,\n",
    "                                                                    scale_factors, lb_scaled, ub_scaled))\n",
    "    # Run MCMC until the acceptance fraction is at least 0.5\n",
    "    sampler.run_mcmc(initial_pos, nsteps, \n",
    "                     progress=False, tune=True)\n",
    "\n",
    "    # Flatten the chain and discard burn-in steps\n",
    "    flat_samples = sampler.get_chain(discard=200, thin=15, flat=True)\n",
    "\n",
    "    # Extract and rescale fit parameters\n",
    "    basal, t_on, t_dwell, rate = np.median(flat_samples, axis=0) * scale_factors\n",
    "\n",
    "    # Calculate confidence intervals\n",
    "    CI = np.percentile(flat_samples, [5, 95], axis=0).T * scale_factors[:, np.newaxis]\n",
    "\n",
    "    return basal, t_on, t_dwell, rate, CI\n",
    "\n",
    "def first_derivative(x, y):\n",
    "    \"\"\"\n",
    "    Compute the first discrete derivative of y with respect to x.\n",
    "    Parameters:\n",
    "    x (numpy.ndarray): Independent variable data points.\n",
    "    y (numpy.ndarray): Dependent variable data points.\n",
    "    Returns:\n",
    "    numpy.ndarray: Discrete first derivative of y with respect to x.\n",
    "    \"\"\"\n",
    "    dx = np.diff(x)\n",
    "    dy = np.diff(y)\n",
    "    dydx = dy / dx\n",
    "\n",
    "    # Use central differences for the interior points and forward/backward differences for the endpoints\n",
    "    dydx_central = np.zeros_like(y)\n",
    "    dydx_central[1:-1] = (y[2:] - y[:-2]) / (x[2:] - x[:-2])\n",
    "    dydx_central[0] = dydx[0]\n",
    "    dydx_central[-1] = dydx[-1]\n",
    "\n",
    "    return dydx_central\n",
    "\n",
    "def mean_sign_intervals(function):\n",
    "    \"\"\"\n",
    "    Compute the mean of function over intervals where the function has a constant sign.\n",
    "    Parameters:\n",
    "    derivative (numpy.ndarray): Array representing the function.\n",
    "    Returns:\n",
    "    numpy.ndarray: Array with mean values of the function over intervals with constant sign.\n",
    "    \"\"\"\n",
    "    # Identify where the sign changes\n",
    "    sign_changes = np.diff(np.sign(function))\n",
    "    # Get indices where the sign changes\n",
    "    change_indices = np.where(sign_changes != 0)[0] + 1\n",
    "\n",
    "    # Initialize the list to hold mean values\n",
    "    mean_values = []\n",
    "    start_index = 0\n",
    "\n",
    "    for end_index in change_indices:\n",
    "        # Calculate the mean of the current interval\n",
    "        interval_mean = np.mean(function[start_index:end_index])\n",
    "        # Append the mean value to the list\n",
    "        mean_values.extend([interval_mean] * (end_index - start_index))\n",
    "        # Update the start index\n",
    "        start_index = end_index\n",
    "\n",
    "    # Handle the last interval\n",
    "    interval_mean = np.mean(function[start_index:])\n",
    "    mean_values.extend([interval_mean] * (len(function) - start_index))\n",
    "\n",
    "    return np.array(mean_values), change_indices\n",
    "\n",
    "# Function to generate fits for all traces\n",
    "def fit_all_traces(traces, tv_denoised_traces):\n",
    "    \"\"\"\n",
    "    Fit half-cycles to all traces in the dataset.\n",
    "    Parameters:\n",
    "    traces (list): List of traces to fit.\n",
    "    tv_denoised_traces (list): List of TV denoised traces.\n",
    "    Returns:\n",
    "    list: List of tuples with fit parameters for each trace.\n",
    "    \"\"\"\n",
    "    # Initialize the list to hold fit results\n",
    "    fit_results = []\n",
    "\n",
    "    # Create new dataframe to store fit results\n",
    "    dataframe = pd.DataFrame(columns=['particle', 'fit_results'])\n",
    "    \n",
    "    for i in range(len(traces)):\n",
    "        # Compute the first derivative of TV denoised with respect to time\n",
    "        dy_dx = first_derivative(traces[i][0], tv_denoised_traces[i])\n",
    "\n",
    "        # Compute the mean of the first derivative over intervals with constant sign\n",
    "        mean_dy_dx, change_indices = mean_sign_intervals(dy_dx)\n",
    "\n",
    "        # Keep datapoints from before first sign change\n",
    "        try:\n",
    "            timepoints = traces[i][0][:change_indices[0]]\n",
    "            MS2 = traces[i][1][:change_indices[0]]\n",
    "            MS2_std = traces[i][2][:change_indices[0]]\n",
    "\n",
    "            # Interpolate the timepoints\n",
    "            t_interp = np.linspace(min(timepoints), max(timepoints), 1000)\n",
    "        except:\n",
    "            print(f\"Failed to find derivative sign change for trace {traces[i][3]}\")\n",
    "            fit_results.append([None, None, None, None, None])\n",
    "            dataframe.loc[i] = [traces[i][3], [None, None, None, None, None]]\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Compute the fit values\n",
    "        try:\n",
    "            basal, t_on, t_dwell, rate, CI = fit_half_cycle(MS2, timepoints, t_interp, MS2_std)\n",
    "            fit_result = [timepoints, t_interp, MS2, make_half_cycle(basal, t_on, t_dwell, rate, t_interp),\n",
    "                                [basal, t_on, t_dwell, rate, CI]]\n",
    "            \n",
    "            fit_results.append(fit_result)\n",
    "            dataframe.loc[i] = [traces[i][3], fit_result]\n",
    "        except:\n",
    "            print(f\"Failed to fit trace {traces[i][3]}\")\n",
    "            fit_results.append([timepoints,t_interp, MS2, None, None])\n",
    "            dataframe.loc[i] = [traces[i][3], [timepoints,t_interp, MS2, None, None]]\n",
    "            continue\n",
    "\n",
    "    return fit_results, dataframe"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Fitting on Ordered Spots"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import medfilt\n",
    "from skimage.restoration import denoise_tv_chambolle\n",
    "\n",
    "# Restrict to longer traces\n",
    "min_frames = 40\n",
    "traces_compiled_dataframe = compiled_dataframe[\n",
    "    compiled_dataframe[\"frame\"].apply(lambda x: x.size) > min_frames\n",
    "]\n",
    "# Restrict to traces starting at frame nc14_frame and above\n",
    "nc14_frame = 438\n",
    "traces_compiled_dataframe = traces_compiled_dataframe[\n",
    "    traces_compiled_dataframe[\"frame\"].apply(lambda x: x[0] >= nc14_frame)\n",
    "]\n",
    "\n",
    "# Order the traces based on the mean x position\n",
    "traces_compiled_dataframe = traces_compiled_dataframe.sort_values(\n",
    "    by=\"ap\", key=lambda x: x.apply(np.mean)\n",
    ")\n",
    "\n",
    "traces = plottable.generate_trace_plot_list(traces_compiled_dataframe)\n",
    "\n",
    "\n",
    "# Generate TV denoised traces\n",
    "tv_denoised_traces = [\n",
    "    denoise_tv_chambolle(trace[1], weight=1080, max_num_iter=500) for trace in traces\n",
    "]\n",
    "\n",
    "# Generate fits for all traces\n",
    "fit_results, dataframe = fit_all_traces(traces, tv_denoised_traces)\n",
    "\n",
    "print(f\"Number of traces: {len(traces)}\")\n",
    "\n",
    "# Show number of traces with valid fits\n",
    "print(f\"Number of traces with valid fits: {sum([result[4] is not None for result in fit_results])}\")\n",
    "\n",
    "# Show number of traces with invalid fits\n",
    "print(f\"Number of traces with invalid fits: {sum([result[4] is None for result in fit_results])}\")\n",
    "\n",
    "traces_compiled_dataframe_fits = pd.merge(traces_compiled_dataframe, dataframe, on='particle', how='inner')\n",
    "\n",
    "# Add columns: the approval status, denoised trace, and fitted rate of the particle\n",
    "length = traces_compiled_dataframe_fits.index.max()\n",
    "status = [1 for _ in range(length+1)]\n",
    "traces_compiled_dataframe_fits['tv_denoised_trace'] = tv_denoised_traces\n",
    "traces_compiled_dataframe_fits['approval_status'] = status\n",
    "\n",
    "traces_compiled_dataframe_fits.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Plot and Inspect "
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from warnings import warn\n",
    "from warnings import warn\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "%matplotlib widget\n",
    "\n",
    "### Josh's Fit checking function\n",
    "def check_particle_fit(binned_particles_fitted):\n",
    "    '''\n",
    "    Check the fit of each particle. All particles are approved by default.\n",
    "    '''\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    particle_index = 0\n",
    "    particle_num = binned_particles_fitted.index.max()\n",
    "\n",
    "    # move to the first unchecked particle--------------------------------------\n",
    "    # first_flag = False\n",
    "    # while not first_flag:\n",
    "    #     particle_data = binned_particles_fitted[particle_index:particle_index+1]\n",
    "    #     status = particle_data['approval_status'].values[0]\n",
    "    #     if status == 0:\n",
    "    #         first_flag = True\n",
    "    #     else:\n",
    "    #         if particle_index < particle_num:\n",
    "    #             particle_index += 1\n",
    "    #         elif particle_index == particle_num:\n",
    "    #             warn('No particle has been left unchecked')\n",
    "    #             break\n",
    "    #---------------------------------------------------------------------------\n",
    "    # import matplotlib\n",
    "    # plt.close('all')\n",
    "    # matplotlib.use('Qt5Agg')\n",
    "    def update_plot(particle_index):\n",
    "        ax.clear()\n",
    "        try:\n",
    "            particle_data = binned_particles_fitted[particle_index:particle_index+1] # select the particle\n",
    "            \n",
    "            x = particle_data['t_s'].values[0]\n",
    "            y = particle_data['intensity_from_neighborhood'].values[0]\n",
    "            y_denoised = particle_data['tv_denoised_trace'].values[0]\n",
    "            y_err = particle_data['intensity_std_error_from_neighborhood'].values[0]\n",
    "    \n",
    "            # plot the particle trace with error bar along with the denoised trace\n",
    "            ax.errorbar(x, y, yerr=y_err, fmt=\".\", elinewidth=1, label='Data')\n",
    "            ax.plot(x, y_denoised, color='k', label='TV denoised')\n",
    "    \n",
    "            # plot the half cycle fit\n",
    "            try:\n",
    "                fit_result = particle_data['fit_results'].values[0]\n",
    "                timepoints, t_interp, MS2, half_cycle_fit, [basal, t_on, t_dwell, rate, CI] = fit_result\n",
    "    \n",
    "                ax.plot(t_interp, half_cycle_fit, label=f'Fit (slope = {round(rate,2)})', linewidth=3)\n",
    "    \n",
    "                # ax.plot(t_interp, make_half_cycle(basal, t_on, t_dwell, rate, t_interp), label=f\"Fit (slope = {round(rate, 2)})\", linewidth=3, color='orange')\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "            particle = particle_data['particle'].values[0]\n",
    "            # bin = particle_data['bin'].values[0]\n",
    "            mean_ap = (particle_data.loc[particle_data[\"particle\"] == particle, \"ap\"]\n",
    "            .values[0]\n",
    "            .mean()\n",
    "            )\n",
    "            initial_frame = (compiled_dataframe.loc[compiled_dataframe[\"particle\"] == particle, \"frame\"].\n",
    "            values[0][0]\n",
    "            )\n",
    "            status = particle_data['approval_status'].values[0]\n",
    "    \n",
    "            if status == 1:\n",
    "                ax.set_facecolor((0.7, 1, 0.7)) # approve\n",
    "            elif status == -1:\n",
    "                ax.set_facecolor((1, 0.7, 0.7)) # reject color\n",
    "            elif status == 0:\n",
    "                ax.set_facecolor((1, 1, 1))\n",
    "            elif status == 2:\n",
    "                ax.set_facecolor((1, 1, 0.7))\n",
    "                \n",
    "            \n",
    "            ax.set_title(f'Particle #{particle} ({particle_index+1}/{particle_num+1}), AP = {np.round(mean_ap, 2)}, Initial frame {initial_frame}')\n",
    "            ax.set_xlabel(\"Time (s)\")\n",
    "            ax.set_ylabel(\"Spot intensity (AU)\")\n",
    "            ax.legend()\n",
    "\n",
    "        except Exception as e:\n",
    "            particle = particle_data['particle'].values[0]\n",
    "            print(f\"Error processing particle {particle}: {e}\")\n",
    "\n",
    "        fig.canvas.draw()\n",
    "\n",
    "    def on_key(event):\n",
    "        nonlocal particle_index\n",
    "        if event.key == 'left':\n",
    "            particle_index = max(0, particle_index - 1)\n",
    "        elif event.key == 'right':\n",
    "            particle_index = min(len(binned_particles_fitted) - 1, particle_index + 1)\n",
    "        elif event.key == 'a':\n",
    "            binned_particles_fitted.at[particle_index, 'approval_status'] = 1\n",
    "        elif event.key == 'r':\n",
    "            binned_particles_fitted.at[particle_index, 'approval_status'] = -1\n",
    "        elif event.key == 'c':\n",
    "            binned_particles_fitted.at[particle_index, 'approval_status'] = 0\n",
    "        elif event.key == 'p':\n",
    "            binned_particles_fitted.at[particle_index, 'approval_status'] = 2\n",
    "        update_plot(particle_index)\n",
    "\n",
    "    update_plot(particle_index)\n",
    "    fig.canvas.mpl_connect('key_press_event', on_key)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "check_particle_fit(traces_compiled_dataframe_fits)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "traces_compiled_dataframe_fits_checked = traces_compiled_dataframe_fits[traces_compiled_dataframe_fits['approval_status'] == 1].reset_index()\n",
    "\n",
    "file_path = test_dataset_name +'/' + 'traces_compiled_dataframe_fits_checked.pkl'\n",
    "print(file_path)\n",
    "# traces_compiled_dataframe_fits_checked.to_pickle(file_path, compression=None)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "file_path = test_dataset_name +'/' + 'traces_compiled_dataframe_fits_checked.pkl'\n",
    "print(file_path)\n",
    "# Load the DataFrame from the .pkl file\n",
    "try:\n",
    "    traces_compiled_dataframe_fits_checked = pd.read_pickle(file_path)\n",
    "    print(\"DataFrame loaded successfully!\")\n",
    "    print(traces_compiled_dataframe_fits_checked.head())  # Display the first few rows of the DataFrame\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the DataFrame: {e}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a DataFrame called 'dataframe' with a column 'mean_x' representing the mean x position\n",
    "# and a column 'rate' representing the fit rates\n",
    "\n",
    "# Define the number of bins and the bin width\n",
    "num_bins = 42\n",
    "bin_width = 1/num_bins\n",
    "\n",
    "# Create an array to store the bin indices for each trace\n",
    "bin_indices = np.zeros(len(traces_compiled_dataframe_fits_checked))\n",
    "\n",
    "# Loop through traces_compiled_dataframe_fits_checked and assign each trace to a bin based on mean_ap position\n",
    "for i in range(len(traces_compiled_dataframe_fits_checked)):\n",
    "    particle = traces_compiled_dataframe_fits_checked['particle'][i]\n",
    "    bin_indices[i] = (\n",
    "        traces_compiled_dataframe_fits_checked.loc[traces_compiled_dataframe_fits_checked['particle'] == particle, 'ap']\n",
    "        .values[0].mean() // bin_width\n",
    "    )\n",
    "\n",
    "# Calculate the number of traces in each bin\n",
    "bin_counts = np.zeros(num_bins)\n",
    "for i in range(num_bins):\n",
    "    bin_counts[i] = np.sum(bin_indices == i)\n",
    "\n",
    "print(f'bin_counts = {bin_counts}')\n",
    "print(f'np.sum(bin_counts) = {np.sum(bin_counts)}')\n",
    "\n",
    "# Calculate the average fit rates for each bin and store particle IDs with rates in each bin\n",
    "mean_fit_rates = np.zeros(num_bins)\n",
    "bin_particles_rates = np.zeros(num_bins, dtype=object)\n",
    "SE_fit_rates = np.zeros(num_bins)\n",
    "for i in range(num_bins):\n",
    "    if bin_counts[i] == 0:\n",
    "        mean_fit_rates[i] = np.nan\n",
    "        continue\n",
    "    else:\n",
    "        rates = (\n",
    "            60*traces_compiled_dataframe_fits_checked.loc[bin_indices == i, 'fit_results'].apply(\n",
    "                lambda x: x[4][3] if x[4] is not None else np.nan).values\n",
    "            )\n",
    "        particles = (\n",
    "            traces_compiled_dataframe_fits_checked.loc[bin_indices == i, 'particle']\n",
    "            .values\n",
    "            )\n",
    "        \n",
    "        # Store the particle IDs with their rates in each bin for further analysis\n",
    "        bin_particles_rates[i] = {\n",
    "            'bin': i,\n",
    "            'particles': particles,\n",
    "            'rates': rates\n",
    "        }\n",
    "        \n",
    "        mean_fit_rates[i] = (np.nanmean(rates))\n",
    "\n",
    "        # Standard error of the mean\n",
    "        SE_fit_rates[i] = np.nanstd(rates) / np.sqrt(len(rates))\n",
    "\n",
    "\n",
    "# Plot the mean slopes for each bin number\n",
    "plt.figure()\n",
    "#plt.errorbar(np.arange(num_bins), mean_fit_rates, yerr=SE_fit_rates, fmt='o')\n",
    "#plt.xlabel('Bin number')\n",
    "plt.errorbar(np.arange(num_bins)/num_bins, mean_fit_rates, yerr=SE_fit_rates, fmt='o')\n",
    "plt.xlabel('AP Position')\n",
    "plt.ylabel('Mean fit rate (AU/min)')\n",
    "plt.ylim(0,120)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bin_particles_rates"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
